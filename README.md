# vLLM æ€§èƒ½æ¸¬è©¦ç³»çµ±

åœ¨ AMD GPU (ROCm) ä¸Šä½¿ç”¨ Docker Compose éƒ¨ç½² vLLM æœå‹™å™¨å’Œå®¢æˆ¶ç«¯é€²è¡Œå…¨é¢æ€§èƒ½æ¸¬è©¦èˆ‡åˆ†æã€‚

**ç¡¬é«”ç’°å¢ƒ**: AMD Radeon AI PRO R9700 (gfx1201)
**Docker æ˜ åƒ**: `rocm/vllm:rocm7.0.0_vllm_0.10.2_20251006`
**vLLM ç‰ˆæœ¬**: 0.10.2

---

## âœ… å·²æ¸¬è©¦æ¨¡å‹åˆ—è¡¨

ä»¥ä¸‹æ¨¡å‹å·²åœ¨æœ¬ç’°å¢ƒä¸­æ¸¬è©¦ï¼Œç¢ºèªå¯æ­£å¸¸é‹è¡Œï¼š

| æ¨¡å‹åç¨± | å®Œæ•´è·¯å¾‘ | æ¸¬è©¦ç‹€æ…‹ | å‚™è¨» |
|---------|---------|---------|------|
| **GPT-OSS 120B** | `openai/gpt-oss-120b` | âœ… æˆåŠŸ | éœ€ 4 GPU (tensor-parallel-size=4) |
| **Gemma 3 4B Instruct** | `google/gemma-3-4b-it` | âœ… æˆåŠŸ | è¼•é‡ç´šæ¨¡å‹ï¼Œé©åˆå¿«é€Ÿæ¸¬è©¦ |
| **Llama 3.1 8B** | `meta-llama/Llama-3.1-8B` | âœ… æˆåŠŸ | Meta å®˜æ–¹æ¨¡å‹ |
| **Qwen 3 8B** | `Qwen/Qwen3-8B` | âœ… æˆåŠŸ | é˜¿é‡Œé€šç¾©åƒå•æ¨¡å‹ |
| **Ministral 3 14B Instruct** | `mistralai/Ministral-3-14b-Instruct-2512` | âŒ å¤±æ•— | éœ€è¦ vLLM â‰¥ 0.12.0ï¼ŒROCm å®˜æ–¹æ˜ åƒå°šæœªæä¾› |

### ä½¿ç”¨æ–¹å¼

ä½¿ç”¨è…³æœ¬çš„ `--model` åƒæ•¸æŒ‡å®šä¸åŒæ¨¡å‹ï¼š

```bash
# ä½¿ç”¨é è¨­æ¨¡å‹ (GPT-OSS 120B)
./run_scaling_bench_200.sh

# ä½¿ç”¨ Gemma 3 4B
./run_scaling_bench_200.sh --model google/gemma-3-4b-it

# ä½¿ç”¨ Llama 3.1 8B
./run_scaling_bench_200.sh --model meta-llama/Llama-3.1-8B

# ä½¿ç”¨ Qwen 3 8B
./run_scaling_bench_200.sh --model Qwen/Qwen3-8B
```

### çµæœæª”æ¡ˆçµ„ç¹”

ä¸åŒæ¨¡å‹çš„æ¸¬è©¦çµæœæœƒè‡ªå‹•å„²å­˜åˆ°å°æ‡‰çš„å­è³‡æ–™å¤¾ï¼š

```
bench_results/scaling/
â”œâ”€â”€ gpt-oss-120b/
â”‚   â”œâ”€â”€ gpt-oss-120b_scale_n1_20251210_120000.json
â”‚   â”œâ”€â”€ gpt-oss-120b_scale_n2_20251210_120100.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ gemma-3-4b-it/
â”‚   â”œâ”€â”€ gemma-3-4b-it_scale_n1_20251210_130000.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ Llama-3.1-8B/
â”‚   â”œâ”€â”€ Llama-3.1-8B_scale_n1_20251210_140000.json
â”‚   â””â”€â”€ ...
â””â”€â”€ Qwen3-8B/
    â”œâ”€â”€ Qwen3-8B_scale_n1_20251210_150000.json
    â””â”€â”€ ...
```

---

## ğŸ“ å°ˆæ¡ˆçµæ§‹

```
vllm_t/
â”‚
â”œâ”€â”€ ğŸ“‚ benchmark_tests/          # vLLM æ¨è«–åŸºæº–æ¸¬è©¦
â”‚   â”œâ”€â”€ scripts/                 # æ¸¬è©¦è…³æœ¬ï¼ˆå®¹å™¨å…§åŸ·è¡Œï¼‰
â”‚   â”‚   â”œâ”€â”€ run_benchmark.sh              # é€šç”¨åŸºæº–æ¸¬è©¦ï¼ˆå¯è‡ªè¨‚åƒæ•¸ï¼‰
â”‚   â”‚   â”œâ”€â”€ run_production_bench.sh       # Production æ¸¬è©¦ï¼ˆ6ç¨®é•·åº¦ Ã— 5ç¨®ä¸¦ç™¼ï¼‰
â”‚   â”‚   â”œâ”€â”€ run_scaling_bench.sh          # æ“´å±•æ€§æ¸¬è©¦ï¼ˆ1-1000 è«‹æ±‚ï¼‰
â”‚   â”‚   â””â”€â”€ run_scaling_bench_200.sh      # æ“´å±•æ€§æ¸¬è©¦ï¼ˆ1-200 è©³ç´°ï¼‰
â”‚   â”‚
â”‚   â””â”€â”€ plot_scripts/            # ç¹ªåœ–å·¥å…·ï¼ˆä¸»æ©Ÿç«¯åŸ·è¡Œï¼‰
â”‚       â”œâ”€â”€ plot_comprehensive_benchmark.py    # ç¶œåˆå ±å‘Šç¹ªåœ–
â”‚       â”œâ”€â”€ plot_normalized_benchmark.py       # æ¨™æº–åŒ–å ±å‘Šç¹ªåœ–
â”‚       â”œâ”€â”€ plot_scaling_benchmark.py          # æ“´å±•æ€§å ±å‘Šç¹ªåœ–ï¼ˆ1-1000ï¼‰
â”‚       â”œâ”€â”€ plot_scaling_benchmark_200.py      # æ“´å±•æ€§å ±å‘Šç¹ªåœ–ï¼ˆ1-200ï¼‰
â”‚       â”œâ”€â”€ run_comprehensive_plot.sh          # ç¶œåˆå ±å‘Šç”Ÿæˆå™¨
â”‚       â”œâ”€â”€ run_normalized_plot.sh             # æ¨™æº–åŒ–å ±å‘Šç”Ÿæˆå™¨
â”‚       â”œâ”€â”€ run_scaling_plot.sh                # æ“´å±•æ€§å ±å‘Šç”Ÿæˆå™¨ï¼ˆ1-1000ï¼‰
â”‚       â””â”€â”€ run_scaling_plot_200.sh            # æ“´å±•æ€§å ±å‘Šç”Ÿæˆå™¨ï¼ˆ1-200ï¼‰
â”‚
â”œâ”€â”€ ğŸ“‚ docker_setup/             # Docker ç’°å¢ƒé…ç½®
â”‚   â”œâ”€â”€ docker-compose.bench.yml     # Docker Compose é…ç½®æ–‡ä»¶
â”‚   â””â”€â”€ README.md                    # Docker è¨­ç½®æ–‡æª”
â”‚
â”œâ”€â”€ ğŸ“‚ bench_results/            # æ¸¬è©¦çµæœï¼ˆè‡ªå‹•ç”Ÿæˆï¼ŒGit å¿½ç•¥ï¼‰
â”‚   â”œâ”€â”€ production/              # Production æ¸¬è©¦çµæœ (JSON)
â”‚   â””â”€â”€ scaling/                 # æ“´å±•æ€§æ¸¬è©¦çµæœ (JSON)
â”‚
â”œâ”€â”€ ğŸ“‚ output_plots/             # ç”Ÿæˆåœ–è¡¨ï¼ˆè‡ªå‹•ç”Ÿæˆï¼‰
â”‚   â”œâ”€â”€ benchmark_comprehensive.png    # ç¶œåˆå ±å‘Šï¼ˆååé‡ã€TTFTã€TPOTï¼‰
â”‚   â”œâ”€â”€ benchmark_normalized.png       # æ¨™æº–åŒ–å ±å‘Š
â”‚   â”œâ”€â”€ scaling_benchmark.png          # æ“´å±•æ€§å ±å‘Šï¼ˆ1-1000ï¼‰
â”‚   â””â”€â”€ scaling_benchmark_200.png      # æ“´å±•æ€§å ±å‘Šï¼ˆ1-200ï¼‰
â”‚
â”œâ”€â”€ ğŸ“‚ docs/                     # é …ç›®æ–‡æª”èˆ‡æ­·å²è³‡æ–™
â”‚   â”œâ”€â”€ troubleshooting/         # æ•…éšœæ’é™¤æŒ‡å—
â”‚   â””â”€â”€ archive/                 # æ­·å²æ–‡æª”
â”‚
â”œâ”€â”€ ğŸ“„ README.md                 # æœ¬æ–‡æª”
â””â”€â”€ ğŸ“„ .gitignore                # Git å¿½ç•¥è¦å‰‡
```

---

## ğŸ¯ æ ¸å¿ƒåŠŸèƒ½

### 1. Production æ¸¬è©¦
æ¸¬è©¦ä¸åŒ context length å’Œä¸¦ç™¼æ•¸çµ„åˆçš„æ€§èƒ½è¡¨ç¾

**æ¸¬è©¦é…ç½®**:
- **è¼¸å…¥é•·åº¦**: 1K, 10K, 32K, 64K, 96K, 128K (å¯åœ¨è…³æœ¬ä¸­èª¿æ•´)
- **è¼¸å‡ºé•·åº¦**: 500 tokens (å›ºå®š)
- **ä¸¦ç™¼æ•¸**: 1, 2, 5, 10, 20 users
- **ç¸½æ¸¬è©¦é»**: 6 Ã— 5 = 30 å€‹æ¸¬è©¦

**æ¸¬è©¦æŒ‡æ¨™**:
- ååé‡ (Throughput)
- é¦– Token æ™‚é–“ (TTFT - Time to First Token)
- æ¯ Token æ™‚é–“ (TPOT - Time per Output Token)

### 2. Scaling æ¸¬è©¦
æ¸¬è©¦ç³»çµ±è™•ç†ä¸åŒè«‹æ±‚æ•¸é‡çš„æ“´å±•æ€§èƒ½åŠ›

**æ¸¬è©¦é…ç½®**:
- **è¼¸å…¥é•·åº¦**: 1K (1024 tokens, å›ºå®š)
- **è¼¸å‡ºé•·åº¦**: 128 tokens (å›ºå®š)
- **è«‹æ±‚æ•¸ç¯„åœ**:
  - 1-1000: è©³ç´°æ¸¬è©¦ (1-200 é€å€‹, 200-1000 æ¯50å€‹)
  - 1-200: æ›´å¯†é›†çš„è©³ç´°æ¸¬è©¦

### 3. è‡ªè¨‚æ¸¬è©¦
ä½¿ç”¨ `run_benchmark.sh` å¯å®Œå…¨è‡ªè¨‚æ¸¬è©¦åƒæ•¸

---

## ğŸš€ å¿«é€Ÿé–‹å§‹

### æ­¥é©Ÿ 1: å•Ÿå‹• Docker ç’°å¢ƒ

```bash
cd docker_setup
docker compose -f docker-compose.bench.yml up -d

# ç¢ºèªå®¹å™¨ç‹€æ…‹
docker compose -f docker-compose.bench.yml ps
```

### æ­¥é©Ÿ 2: å•Ÿå‹• vLLM æœå‹™å™¨

**çµ‚ç«¯ 1** - æœå‹™å™¨ç«¯ï¼š

```bash
# é€²å…¥æœå‹™å™¨å®¹å™¨
docker exec -it vllm-server bash

# å•Ÿå‹• vLLM æœå‹™å™¨ï¼ˆ120B å¤§æ¨¡å‹ï¼‰
vllm serve openai/gpt-oss-120b \
  --tensor-parallel-size 4 \
  --gpu-memory-utilization 0.9 \
  --enforce-eager

# æˆ–å•Ÿå‹•å°æ¨¡å‹é€²è¡Œæ¸¬è©¦
vllm serve facebook/opt-125m \
  --tensor-parallel-size 1 \
  --gpu-memory-utilization 0.9 \
  --enforce-eager
```

**é‡è¦æç¤º**:
- `--enforce-eager` æ˜¯ AMD GPU å¿…éœ€çš„åƒæ•¸
- ä¸å»ºè­°æ‰‹å‹•è¨­ç½® `--dtype` å’Œ `--max-model-len`ï¼Œè®“ vLLM è‡ªå‹•åµæ¸¬
- `--tensor-parallel-size` æ‡‰è¨­ç‚ºä½¿ç”¨çš„ GPU æ•¸é‡

### æ­¥é©Ÿ 3: é‹è¡ŒåŸºæº–æ¸¬è©¦

**çµ‚ç«¯ 2** - å®¢æˆ¶ç«¯ï¼š

#### é¸é … A: Production æ¸¬è©¦ï¼ˆæ¨è–¦ï¼‰

æ¸¬è©¦å¤šç¨® context length å’Œä¸¦ç™¼æ•¸çµ„åˆï¼š

```bash
docker exec vllm-bench-client bash /root/benchmark_tests/scripts/run_production_bench.sh
```

**åŸ·è¡Œæ™‚é–“**: ç´„ 30-60 åˆ†é˜ï¼ˆå–æ±ºæ–¼æ¨¡å‹å¤§å°å’Œç¡¬é«”ï¼‰
**ç”Ÿæˆæ–‡ä»¶**: `bench_results/production/input_{length}_n{concurrency}_{timestamp}.json`

#### é¸é … B: Scaling æ¸¬è©¦

æ¸¬è©¦è«‹æ±‚æ•¸æ“´å±•æ€§ï¼š

```bash
# æ¸¬è©¦ 1-1000 è«‹æ±‚æ•¸ç¯„åœï¼ˆé è¨­ä½¿ç”¨ gpt-oss-120bï¼‰
docker exec vllm-bench-client bash /root/benchmark_tests/scripts/run_scaling_bench.sh

# æ¸¬è©¦ 1-200 è«‹æ±‚æ•¸ç¯„åœï¼ˆæ›´å¯†é›†ï¼Œé è¨­ä½¿ç”¨ gpt-oss-120bï¼‰
docker exec vllm-bench-client bash /root/benchmark_tests/scripts/run_scaling_bench_200.sh

# æŒ‡å®šä¸åŒæ¨¡å‹é€²è¡Œæ¸¬è©¦
docker exec vllm-bench-client bash -c "cd /root && /root/benchmark_tests/scripts/run_scaling_bench_200.sh --model google/gemma-3-4b-it"
docker exec vllm-bench-client bash -c "cd /root && /root/benchmark_tests/scripts/run_scaling_bench_200.sh --model meta-llama/Llama-3.1-8B"
docker exec vllm-bench-client bash -c "cd /root && /root/benchmark_tests/scripts/run_scaling_bench_200.sh --model Qwen/Qwen3-8B"
```

**åŸ·è¡Œæ™‚é–“**: ç´„ 1-3 å°æ™‚
**ç”Ÿæˆæ–‡ä»¶**: `bench_results/scaling/{model_name}/{model_name}_scale_n{num_prompts}_{timestamp}.json`

#### é¸é … C: è‡ªè¨‚æ¸¬è©¦

```bash
# é€²å…¥å®¢æˆ¶ç«¯å®¹å™¨
docker exec -it vllm-bench-client bash

# æŸ¥çœ‹å®Œæ•´åƒæ•¸èªªæ˜
/root/benchmark_tests/scripts/run_benchmark.sh --help

# ç¯„ä¾‹ï¼šæ¸¬è©¦å–®ä¸€ä¸¦ç™¼æ•¸
/root/benchmark_tests/scripts/run_benchmark.sh --single 8

# ç¯„ä¾‹ï¼šè‡ªè¨‚è¼¸å…¥è¼¸å‡ºé•·åº¦
/root/benchmark_tests/scripts/run_benchmark.sh \
  --input-len 2048 \
  --output-len 256 \
  --min-concurrency 1 \
  --max-concurrency 10
```

### æ­¥é©Ÿ 4: ç”Ÿæˆæ€§èƒ½åœ–è¡¨

åœ¨**ä¸»æ©Ÿç«¯**é‹è¡Œç¹ªåœ–è…³æœ¬ï¼ˆä¸éœ€é€²å…¥å®¹å™¨ï¼‰ï¼š

```bash
cd benchmark_tests/plot_scripts

# Production æ¸¬è©¦åœ–è¡¨
./run_comprehensive_plot.sh      # ç¶œåˆå ±å‘Šï¼ˆååé‡ã€TTFTã€TPOT ä¸‰åˆä¸€ï¼‰
./run_normalized_plot.sh          # æ¨™æº–åŒ–å ±å‘Šï¼ˆç›¸å°åŸºæº–æ€§èƒ½ï¼‰

# Scaling æ¸¬è©¦åœ–è¡¨
./run_scaling_plot.sh             # 1-1000 ç¯„åœ
./run_scaling_plot_200.sh         # 1-200 ç¯„åœ
```

**ç”Ÿæˆä½ç½®**: `output_plots/` ç›®éŒ„

### æ­¥é©Ÿ 5: æŸ¥çœ‹çµæœ

```bash
# æŸ¥çœ‹æ¸¬è©¦çµæœæ–‡ä»¶
ls -lh bench_results/production/*.json
ls -lh bench_results/scaling/*.json

# æŸ¥çœ‹ç”Ÿæˆçš„åœ–è¡¨
ls -lh output_plots/*.png

# ç¾åŒ– JSON è¼¸å‡º
python3 -m json.tool bench_results/production/input_10K_n5_*.json
```

---

## ğŸ“Š æ•¸æ“šæµèˆ‡æ¶æ§‹

### å·¥ä½œæµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. å•Ÿå‹•ç’°å¢ƒ                                                  â”‚
â”‚    docker compose up -d                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. å•Ÿå‹• vLLM Server (å®¹å™¨å…§)                                â”‚
â”‚    vllm serve <model> --tensor-parallel-size 4 ...          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. é‹è¡Œæ¸¬è©¦ (vllm-bench-client å®¹å™¨)                        â”‚
â”‚    run_production_bench.sh / run_scaling_bench.sh           â”‚
â”‚    â”‚                                                         â”‚
â”‚    â”œâ”€â†’ ç™¼é€ä¸¦ç™¼è«‹æ±‚åˆ° vllm-server:8000                      â”‚
â”‚    â”œâ”€â†’ æ”¶é›†æ€§èƒ½æŒ‡æ¨™                                         â”‚
â”‚    â””â”€â†’ ä¿å­˜åˆ° bench_results/*.json                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. ç”Ÿæˆåœ–è¡¨ (ä¸»æ©Ÿç«¯)                                        â”‚
â”‚    run_comprehensive_plot.sh / run_scaling_plot.sh          â”‚
â”‚    â”‚                                                         â”‚
â”‚    â”œâ”€â†’ è®€å– bench_results/*.json                            â”‚
â”‚    â”œâ”€â†’ ä½¿ç”¨ Docker é‹è¡Œ Python/matplotlib                   â”‚
â”‚    â””â”€â†’ ç”Ÿæˆ output_plots/*.png                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ç³»çµ±æ¶æ§‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ä¸»æ©Ÿ (Host Machine)                                            â”‚
â”‚                                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  vllm-bench-client (Container)                           â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
â”‚  â”‚  â”‚ Benchmark Scripts                                  â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ - run_production_bench.sh                          â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ - run_scaling_bench.sh                             â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ - run_benchmark.sh                                 â”‚  â”‚ â”‚
â”‚  â”‚  â”‚                                                     â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ vllm bench serve --backend openai ...              â”‚  â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                       â”‚ HTTP Requests                         â”‚
â”‚                       â”‚ (vllm-network bridge)                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  vllm-server (Container)                                 â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
â”‚  â”‚  â”‚ vLLM API Server                                    â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ - Port 8000                                        â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ - OpenAI Compatible API                            â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ - AMD GPU Access (ROCm)                            â”‚  â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                â”‚
â”‚  ğŸ“ Shared Volumes:                                            â”‚
â”‚  â”œâ”€ ~/.cache/huggingface  â†’ Model cache                       â”‚
â”‚  â”œâ”€ benchmark_tests/scripts â†’ Test scripts                    â”‚
â”‚  â””â”€ bench_results          â†’ Test results (JSON)              â”‚
â”‚                                                                â”‚
â”‚  ğŸ–¥ï¸  Host-side Tools:                                          â”‚
â”‚  â””â”€ benchmark_tests/plot_scripts â†’ Chart generation (PNG)     â”‚
â”‚                                                                â”‚
â”‚  ğŸ”§ GPU Hardware:                                              â”‚
â”‚  â””â”€ /dev/kfd, /dev/dri â†’ AMD ROCm GPU devices                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ å¸¸ç”¨å‘½ä»¤åƒè€ƒ

### Docker å®¹å™¨ç®¡ç†

```bash
# å•Ÿå‹•å®¹å™¨ï¼ˆå¾ docker_setup ç›®éŒ„ï¼‰
cd docker_setup
docker compose -f docker-compose.bench.yml up -d

# æŸ¥çœ‹å®¹å™¨ç‹€æ…‹
docker compose -f docker-compose.bench.yml ps

# æŸ¥çœ‹æ—¥èªŒ
docker compose -f docker-compose.bench.yml logs -f vllm-server
docker compose -f docker-compose.bench.yml logs -f vllm-bench-client

# åœæ­¢å®¹å™¨
docker compose -f docker-compose.bench.yml stop

# åˆªé™¤å®¹å™¨
docker compose -f docker-compose.bench.yml down

# é‡å•Ÿå®¹å™¨
docker compose -f docker-compose.bench.yml restart
```

### é€²å…¥å®¹å™¨

```bash
# é€²å…¥æœå‹™å™¨å®¹å™¨
docker exec -it vllm-server bash

# é€²å…¥å®¢æˆ¶ç«¯å®¹å™¨
docker exec -it vllm-bench-client bash
```

### vLLM æœå‹™å™¨æ“ä½œ

```bash
# åœ¨æœå‹™å™¨å®¹å™¨å…§å•Ÿå‹• vLLM
vllm serve openai/gpt-oss-120b \
  --tensor-parallel-size 4 \
  --gpu-memory-utilization 0.9 \
  --enforce-eager

# æª¢æŸ¥æœå‹™å™¨ç‹€æ…‹ï¼ˆå¾å®¢æˆ¶ç«¯å®¹å™¨æˆ–ä¸»æ©Ÿï¼‰
curl http://localhost:8000/health
curl http://vllm-server:8000/v1/models

# ç›£æ§ GPU ä½¿ç”¨ï¼ˆåœ¨æœå‹™å™¨å®¹å™¨å…§ï¼‰
watch -n 1 rocm-smi
```

### æ¨è«–æ¸¬è©¦å‘½ä»¤

```bash
# Production æ¸¬è©¦ï¼ˆåœ¨ä¸»æ©Ÿç«¯åŸ·è¡Œï¼‰
docker exec vllm-bench-client bash /root/benchmark_tests/scripts/run_production_bench.sh

# Scaling æ¸¬è©¦ï¼ˆé è¨­æ¨¡å‹ï¼‰
docker exec vllm-bench-client bash /root/benchmark_tests/scripts/run_scaling_bench.sh
docker exec vllm-bench-client bash /root/benchmark_tests/scripts/run_scaling_bench_200.sh

# Scaling æ¸¬è©¦ï¼ˆæŒ‡å®šæ¨¡å‹ï¼‰
docker exec vllm-bench-client bash -c "cd /root && /root/benchmark_tests/scripts/run_scaling_bench_200.sh --model google/gemma-3-4b-it"
docker exec vllm-bench-client bash -c "cd /root && /root/benchmark_tests/scripts/run_scaling_bench_200.sh --model Qwen/Qwen3-8B"

# è‡ªè¨‚æ¸¬è©¦ï¼ˆé€²å…¥å®¹å™¨å¾ŒåŸ·è¡Œï¼‰
docker exec -it vllm-bench-client bash
/root/benchmark_tests/scripts/run_benchmark.sh --single 8
```

### ç”Ÿæˆåœ–è¡¨å‘½ä»¤

```bash
# åœ¨ä¸»æ©Ÿç«¯åŸ·è¡Œï¼ˆéœ€åœ¨å°ˆæ¡ˆæ ¹ç›®éŒ„ï¼‰
cd benchmark_tests/plot_scripts

# Production æ¸¬è©¦åœ–è¡¨
./run_comprehensive_plot.sh      # ç¶œåˆå ±å‘Š
./run_normalized_plot.sh          # æ¨™æº–åŒ–å ±å‘Š

# Scaling æ¸¬è©¦åœ–è¡¨
./run_scaling_plot.sh             # 1-1000 ç¯„åœ
./run_scaling_plot_200.sh         # 1-200 ç¯„åœ
```

### çµæœæŸ¥çœ‹èˆ‡ç®¡ç†

```bash
# æŸ¥çœ‹æœ€è¿‘çš„æ¸¬è©¦çµæœ
ls -lt bench_results/production/*.json | head -10
ls -lt bench_results/scaling/*.json | head -10

# æŸ¥çœ‹ç”Ÿæˆçš„åœ–è¡¨
ls -lh output_plots/*.png

# ç¾åŒ– JSON è¼¸å‡º
python3 -m json.tool bench_results/production/input_10K_n5_*.json | less

# æ¸…ç†èˆŠçµæœï¼ˆè¬¹æ…ä½¿ç”¨ï¼‰
rm -rf bench_results/production/*.json
rm -rf bench_results/scaling/*.json
rm -rf output_plots/*.png
```

---

## âš™ï¸ é…ç½®èªªæ˜

### Production æ¸¬è©¦é…ç½®

ç·¨è¼¯ [benchmark_tests/scripts/run_production_bench.sh](benchmark_tests/scripts/run_production_bench.sh):

```bash
# ä¸¦ç™¼æ•¸ç´šåˆ¥ï¼ˆç¬¬ 22 è¡Œï¼‰
NUM_PROMPTS_LEVELS=(1 2 5 10 20)

# è¼¸å…¥é•·åº¦é…ç½®ï¼ˆç¬¬ 27-34 è¡Œï¼‰
INPUT_CONFIGS=(
    "1024:1K"
    "10240:10K"
    "32768:32K"
    "65536:64K"
    "98304:96K"
    "131072:128K"
)

# è¼¸å‡ºé•·åº¦ï¼ˆç¬¬ 23 è¡Œï¼‰
OUTPUT_LEN=500
```

### Scaling æ¸¬è©¦é…ç½®

ç·¨è¼¯ [benchmark_tests/scripts/run_scaling_bench.sh](benchmark_tests/scripts/run_scaling_bench.sh):

```bash
# è¼¸å…¥é•·åº¦ï¼ˆç¬¬ 23-24 è¡Œï¼‰
INPUT_LEN=1024
INPUT_LABEL="1K"

# è¼¸å‡ºé•·åº¦ï¼ˆç¬¬ 25 è¡Œï¼‰
OUTPUT_LEN=128

# è«‹æ±‚æ•¸æ¸¬è©¦é»ï¼ˆç¬¬ 29 è¡Œï¼‰
NUM_PROMPTS_LEVELS=(1 2 3 4 ... 200 250 300 ... 1000)
```

### vLLM æœå‹™å™¨åƒæ•¸

| åƒæ•¸ | èªªæ˜ | æ¨è–¦å€¼ |
|------|------|--------|
| `--tensor-parallel-size` | GPU æ•¸é‡ | æ ¹æ“šæ¨¡å‹å¤§å°ï¼š4 (120B), 1 (å°æ¨¡å‹) |
| `--gpu-memory-utilization` | GPU è¨˜æ†¶é«”ä½¿ç”¨ç‡ | 0.9 (90%) |
| `--enforce-eager` | AMD GPU å¿…éœ€ | å¿…é ˆåŒ…å« |
| `--dtype` | è³‡æ–™é¡å‹ | ä¸è¨­å®šï¼ˆè‡ªå‹•åµæ¸¬ï¼‰ |
| `--max-model-len` | æœ€å¤§ä¸Šä¸‹æ–‡é•·åº¦ | ä¸è¨­å®šï¼ˆè‡ªå‹•åµæ¸¬ï¼‰ |

---

## ğŸ’¡ å¯¦ç”¨æŠ€å·§

### 1. æ¸¬è©¦å‰æª¢æŸ¥

```bash
# æª¢æŸ¥æœå‹™å™¨æ˜¯å¦å•Ÿå‹•
curl http://localhost:8000/health

# æª¢æŸ¥ GPU ç‹€æ…‹
docker exec vllm-server rocm-smi

# æª¢æŸ¥ç¶²è·¯é€£æ¥ï¼ˆå¾å®¢æˆ¶ç«¯ï¼‰
docker exec vllm-bench-client curl http://vllm-server:8000/health
```

### 2. ç›£æ§æ¸¬è©¦é€²åº¦

```bash
# å³æ™‚æŸ¥çœ‹å®¢æˆ¶ç«¯æ—¥èªŒ
docker logs -f vllm-bench-client

# å³æ™‚æŸ¥çœ‹æœå‹™å™¨æ—¥èªŒ
docker logs -f vllm-server

# ç›£æ§ GPU ä½¿ç”¨ï¼ˆæœå‹™å™¨å®¹å™¨å…§ï¼‰
docker exec -it vllm-server bash
watch -n 1 rocm-smi
```

### 3. çµæœåˆ†ææŠ€å·§

```bash
# çµ±è¨ˆæŸå€‹é…ç½®çš„æ¸¬è©¦æ•¸é‡
ls bench_results/production/input_10K_* | wc -l

# æŸ¥æ‰¾ç‰¹å®šä¸¦ç™¼æ•¸çš„çµæœ
ls bench_results/production/input_*_n5_*.json

# æå–é—œéµæŒ‡æ¨™ï¼ˆä½¿ç”¨ jqï¼‰
cat bench_results/production/input_10K_n5_*.json | \
  jq '{throughput: .throughput, ttft: .ttft, tpot: .tpot}'
```

### 4. æ¸¬è©¦æœ€ä½³å¯¦è¸

1. **å¾å°åˆ°å¤§æ¸¬è©¦**: å…ˆç”¨å°æ¨¡å‹é©—è­‰ç’°å¢ƒï¼Œå†æ¸¬å¤§æ¨¡å‹
2. **æ®µè½å¼æ¸¬è©¦**: ä¸è¦ä¸€æ¬¡é‹è¡Œæ‰€æœ‰æ¸¬è©¦ï¼Œåˆ†æ®µé€²è¡Œé¿å…å•é¡Œç´¯ç©
3. **ä¿ç•™æ—¥èªŒ**: ä¿å­˜æ¸¬è©¦æ—¥èªŒä»¥ä¾¿å¾ŒçºŒå•é¡Œæ’æŸ¥
4. **å®šæœŸæ¸…ç†**: æ¸¬è©¦å‰æ¸…ç†èˆŠçµæœé¿å…æ··æ·†

```bash
# ä¿å­˜æ¸¬è©¦æ—¥èªŒç¯„ä¾‹
docker exec vllm-bench-client bash /root/benchmark_tests/scripts/run_production_bench.sh \
  2>&1 | tee production_test_$(date +%Y%m%d_%H%M%S).log
```

---

## ğŸ” æ•…éšœæ’æŸ¥

### å•é¡Œ 1: å®¹å™¨ç„¡æ³•å•Ÿå‹•

**ç—‡ç‹€**: `docker compose up -d` å¤±æ•—

**æª¢æŸ¥æ–¹å¼**:
```bash
# æŸ¥çœ‹è©³ç´°æ—¥èªŒ
docker compose -f docker_setup/docker-compose.bench.yml logs

# æª¢æŸ¥ GPU è¨­å‚™
ls -l /dev/kfd /dev/dri

# æª¢æŸ¥ Docker ç‰ˆæœ¬
docker compose version
```

**å¸¸è¦‹åŸå› **:
- GPU è¨­å‚™æ¬Šé™å•é¡Œ â†’ åŠ å…¥ `video` ç¾¤çµ„
- Docker Compose ç‰ˆæœ¬éèˆŠ â†’ å‡ç´šåˆ° 2.0+
- ç«¯å£ 8000 è¢«å ç”¨ â†’ ä¿®æ”¹ docker-compose.yml ç«¯å£æ˜ å°„

### å•é¡Œ 2: vLLM æœå‹™å™¨å•Ÿå‹•å¤±æ•—

**ç—‡ç‹€**: æœå‹™å™¨å®¹å™¨å…§ `vllm serve` å‘½ä»¤å¤±æ•—

**æª¢æŸ¥æ–¹å¼**:
```bash
# æŸ¥çœ‹è©³ç´°éŒ¯èª¤
docker logs vllm-server

# æª¢æŸ¥ GPU å¯ç”¨æ€§
docker exec vllm-server rocm-smi
docker exec vllm-server rocminfo

# æª¢æŸ¥æ¨¡å‹æ˜¯å¦ä¸‹è¼‰
docker exec vllm-server ls -lh /root/.cache/huggingface/hub/
```

**å¸¸è¦‹åŸå› **:
- **GPU è¨˜æ†¶é«”ä¸è¶³**: é™ä½ `--gpu-memory-utilization` æˆ–ä½¿ç”¨æ›´å°çš„æ¨¡å‹
- **æ¨¡å‹æœªä¸‹è¼‰**: ç­‰å¾…æ¨¡å‹ä¸‹è¼‰å®Œæˆï¼ˆé¦–æ¬¡éœ€è¦è¼ƒé•·æ™‚é–“ï¼‰
- **ROCm ç‰ˆæœ¬ä¸ç›¸å®¹**: ç¢ºèªä½¿ç”¨æ­£ç¢ºçš„ Docker æ˜ åƒ

**è§£æ±ºæ–¹æ¡ˆ**:
```bash
# GPU è¨˜æ†¶é«”ä¸è¶³
vllm serve openai/gpt-oss-120b \
  --tensor-parallel-size 4 \
  --gpu-memory-utilization 0.7 \  # é™ä½åˆ° 70%
  --enforce-eager

# ä½¿ç”¨å°æ¨¡å‹æ¸¬è©¦
vllm serve facebook/opt-125m \
  --tensor-parallel-size 1 \
  --gpu-memory-utilization 0.9 \
  --enforce-eager
```

### å•é¡Œ 3: å®¢æˆ¶ç«¯ç„¡æ³•é€£æ¥æœå‹™å™¨

**ç—‡ç‹€**: æ¸¬è©¦è…³æœ¬å ±éŒ¯ "ç„¡æ³•é€£æ¥åˆ° vLLM æœå‹™å™¨"

**æª¢æŸ¥æ–¹å¼**:
```bash
# æª¢æŸ¥æœå‹™å™¨å¥åº·ç‹€æ…‹
docker exec vllm-bench-client curl http://vllm-server:8000/health

# æª¢æŸ¥ç¶²è·¯é€£æ¥
docker network inspect docker_setup_vllm-network

# æª¢æŸ¥æœå‹™å™¨æ˜¯å¦åœ¨ç›£è½
docker exec vllm-server netstat -tulpn | grep 8000
```

**å¸¸è¦‹åŸå› **:
- vLLM æœå‹™å™¨æœªå•Ÿå‹•æˆ–å•Ÿå‹•å¤±æ•—
- ç¶²è·¯é…ç½®éŒ¯èª¤
- å®¹å™¨åç¨±ä¸åŒ¹é…

### å•é¡Œ 4: æ¸¬è©¦çµæœæœªç”Ÿæˆ

**ç—‡ç‹€**: æ¸¬è©¦å®Œæˆä½†æ‰¾ä¸åˆ° JSON æ–‡ä»¶

**æª¢æŸ¥æ–¹å¼**:
```bash
# æŸ¥çœ‹æ¸¬è©¦è…³æœ¬è¼¸å‡º
docker logs vllm-bench-client

# æª¢æŸ¥çµæœç›®éŒ„
docker exec vllm-bench-client ls -lh /root/bench_results/production/
docker exec vllm-bench-client ls -lh /root/bench_results/scaling/

# æŸ¥çœ‹ç•¶å‰ç›®éŒ„æ˜¯å¦æœ‰æ®˜ç•™æ–‡ä»¶
docker exec vllm-bench-client ls -lh /root/*.json
```

**å¸¸è¦‹åŸå› **:
- æ¸¬è©¦æœªå®Œæˆï¼ˆè¢«ä¸­æ–·ï¼‰
- çµæœç›®éŒ„æ¬Šé™å•é¡Œ
- vllm bench å‘½ä»¤å¤±æ•—

**è§£æ±ºæ–¹æ¡ˆ**:
```bash
# ç¢ºä¿çµæœç›®éŒ„å­˜åœ¨ä¸”æœ‰æ¬Šé™
docker exec vllm-bench-client mkdir -p /root/bench_results/production
docker exec vllm-bench-client chmod 777 /root/bench_results/production

# æŸ¥çœ‹æ¸¬è©¦é€€å‡ºä»£ç¢¼ï¼ˆåœ¨è…³æœ¬è¼¸å‡ºä¸­ï¼‰
# é€€å‡ºä»£ç¢¼æ‡‰ç‚º 0
```

### å•é¡Œ 5: åœ–è¡¨ç”Ÿæˆå¤±æ•—

**ç—‡ç‹€**: `run_comprehensive_plot.sh` ç­‰è…³æœ¬åŸ·è¡Œå¤±æ•—

**æª¢æŸ¥æ–¹å¼**:
```bash
# æª¢æŸ¥æ¸¬è©¦çµæœæ˜¯å¦å­˜åœ¨
ls -lh bench_results/production/*.json
ls -lh bench_results/scaling/*.json

# æª¢æŸ¥ Docker æ˜¯å¦å¯ç”¨
docker ps

# æ‰‹å‹•æ¸¬è©¦ Python ç’°å¢ƒ
docker run --rm python:3.11-slim python3 --version
```

**å¸¸è¦‹åŸå› **:
- æ¸¬è©¦çµæœæ–‡ä»¶ä¸å­˜åœ¨æˆ–è·¯å¾‘éŒ¯èª¤
- Docker ç„¡æ³•é‹è¡Œ
- Python è…³æœ¬èªæ³•éŒ¯èª¤

**è§£æ±ºæ–¹æ¡ˆ**:
```bash
# ç¢ºä¿åœ¨æ­£ç¢ºçš„ç›®éŒ„åŸ·è¡Œ
cd /home/user/vllm_t/benchmark_tests/plot_scripts
pwd  # æ‡‰è¼¸å‡º /home/user/vllm_t/benchmark_tests/plot_scripts

# æª¢æŸ¥æ¸¬è©¦çµæœè·¯å¾‘
ls -l ../../bench_results/production/

# æŸ¥çœ‹è©³ç´°éŒ¯èª¤ä¿¡æ¯
./run_comprehensive_plot.sh 2>&1 | tee plot_error.log
```

### å•é¡Œ 6: ä¸Šä¸‹æ–‡é•·åº¦è¶…å‡ºé™åˆ¶

**ç—‡ç‹€**: æ¸¬è©¦å ±éŒ¯ "maximum context length is XXX tokens"

**åŸå› **: `input_len + output_len` è¶…éæ¨¡å‹æ”¯æŒçš„æœ€å¤§é•·åº¦

**è§£æ±ºæ–¹æ¡ˆ**:
```bash
# æ–¹æ¡ˆ 1: èª¿æ•´æ¸¬è©¦åƒæ•¸ï¼ˆåœ¨æ¸¬è©¦è…³æœ¬ä¸­ä¿®æ”¹ï¼‰
# ç¢ºä¿ input_len + output_len < max_model_len

# æ–¹æ¡ˆ 2: å•Ÿå‹•æœå‹™å™¨æ™‚è¨­ç½®æ›´å¤§çš„ä¸Šä¸‹æ–‡ï¼ˆä¸æ¨è–¦ï¼‰
vllm serve openai/gpt-oss-120b \
  --tensor-parallel-size 4 \
  --gpu-memory-utilization 0.7 \
  --max-model-len 4096 \
  --enforce-eager
```

### å•é¡Œ 7: ç‰¹å®šæ¨¡å‹ç„¡æ³•è¼‰å…¥

**ç—‡ç‹€**: Ministral 3 14B Instruct å ±éŒ¯ "Tokenizer class TokenizersBackend does not exist"

**éŒ¯èª¤è¨Šæ¯**:
```
ValueError: Tokenizer class TokenizersBackend does not exist or is not currently imported.
RuntimeError: Failed to load the tokenizer.
```

**åŸå› **:
- Ministral 3 14B æ¨¡å‹éœ€è¦ vLLM 0.12.0 æˆ–æ›´æ–°ç‰ˆæœ¬
- ç•¶å‰ä½¿ç”¨çš„ ROCm å®˜æ–¹æ˜ åƒ `rocm/vllm:rocm7.0.0_vllm_0.10.2_20251006` åƒ…æ”¯æ´ vLLM 0.10.2
- ROCm å®˜æ–¹å°šæœªæä¾›åŒ…å« vLLM 0.12.0 çš„æ˜ åƒæª”

**è§£æ±ºæ–¹æ¡ˆ**:
- **æš«æ™‚ç„¡æ³•è§£æ±º**: ç­‰å¾… ROCm å®˜æ–¹ç™¼å¸ƒåŒ…å« vLLM 0.12.0+ çš„æ–°æ˜ åƒ
- **æ›¿ä»£æ–¹æ¡ˆ**: ä½¿ç”¨å…¶ä»–å·²æ¸¬è©¦æˆåŠŸçš„æ¨¡å‹ï¼š
  - `google/gemma-3-4b-it` - è¼•é‡ç´šï¼Œé©åˆå¿«é€Ÿæ¸¬è©¦
  - `meta-llama/Llama-3.1-8B` - Meta å®˜æ–¹æ¨¡å‹
  - `Qwen/Qwen3-8B` - é˜¿é‡Œé€šç¾©åƒå•æ¨¡å‹
  - `openai/gpt-oss-120b` - å¤§å‹æ¨¡å‹ï¼ˆéœ€ 4 GPUï¼‰

**æª¢æŸ¥ vLLM ç‰ˆæœ¬**:
```bash
docker exec vllm-server vllm --version
# è¼¸å‡º: vllm 0.10.2
```

---

## ğŸ“š è©³ç´°æ–‡æª”

- **[docker_setup/README.md](docker_setup/README.md)** - Docker ç’°å¢ƒé…ç½®è©³ç´°èªªæ˜
  - å®¹å™¨é…ç½®
  - ç¶²è·¯è¨­ç½®
  - æ•…éšœæ’é™¤

- **[docs/](docs/)** - å…¶ä»–æ–‡æª”
  - `troubleshooting/` - æ­·å²æ•…éšœæ’é™¤è¨˜éŒ„
  - `archive/` - èˆŠç‰ˆæ–‡æª”å­˜æª”

---

## ğŸ“ˆ æ¸¬è©¦çµæœç¤ºä¾‹

### Production æ¸¬è©¦çµæœ

ç”Ÿæˆæ–‡ä»¶æ ¼å¼: `input_{length}_n{concurrency}_{timestamp}.json`

ç¯„ä¾‹:
```
bench_results/production/
â”œâ”€â”€ input_1K_n1_20251114_120000.json
â”œâ”€â”€ input_1K_n2_20251114_120100.json
â”œâ”€â”€ input_1K_n5_20251114_120200.json
â”œâ”€â”€ input_10K_n1_20251114_120300.json
â”œâ”€â”€ input_10K_n2_20251114_120400.json
â””â”€â”€ ...
```

### Scaling æ¸¬è©¦çµæœ

ç”Ÿæˆæ–‡ä»¶æ ¼å¼: `scale_n{num_prompts}_{timestamp}.json`

ç¯„ä¾‹:
```
bench_results/scaling/
â”œâ”€â”€ scale_n1_20251114_130000.json
â”œâ”€â”€ scale_n2_20251114_130010.json
â”œâ”€â”€ scale_n5_20251114_130020.json
â”œâ”€â”€ scale_n10_20251114_130030.json
â””â”€â”€ ...
```

### åœ–è¡¨è¼¸å‡º

```
output_plots/
â”œâ”€â”€ benchmark_comprehensive.png      # ç¶œåˆå ±å‘Šï¼ˆååé‡ã€TTFTã€TPOTï¼‰
â”œâ”€â”€ benchmark_normalized.png         # æ¨™æº–åŒ–å ±å‘Š
â”œâ”€â”€ scaling_benchmark.png            # æ“´å±•æ€§ï¼ˆ1-1000ï¼‰
â””â”€â”€ scaling_benchmark_200.png        # æ“´å±•æ€§ï¼ˆ1-200ï¼‰
```

---

## âš ï¸ æ³¨æ„äº‹é …

1. **GPU è¨˜æ†¶é«”ç®¡ç†**
   - 120B æ¨¡å‹éœ€è¦ 4 å€‹ GPU ä¸”æ¯å€‹æœ‰è¶³å¤  VRAM
   - å»ºè­°å…ˆç”¨å°æ¨¡å‹ï¼ˆå¦‚ opt-125mï¼‰é©—è­‰ç’°å¢ƒ

2. **æ¸¬è©¦æ™‚é–“è¦åŠƒ**
   - Production å®Œæ•´æ¸¬è©¦éœ€ 30-60 åˆ†é˜
   - Scaling å®Œæ•´æ¸¬è©¦éœ€ 1-3 å°æ™‚
   - å»ºè­°ä½¿ç”¨ `screen` æˆ– `tmux` é¿å…é€£ç·šä¸­æ–·

3. **çµæœç®¡ç†**
   - æ¸¬è©¦çµæœè‡ªå‹•ä¿å­˜åˆ° `bench_results/`
   - `.gitignore` å·²è¨­å®šå¿½ç•¥çµæœæ–‡ä»¶
   - å®šæœŸæ¸…ç†é¿å…ç£ç¢Ÿç©ºé–“ä¸è¶³

4. **Docker ç‰ˆæœ¬**
   - ä½¿ç”¨ `docker compose`ï¼ˆv2ï¼‰è€Œé `docker-compose`ï¼ˆv1ï¼‰
   - Compose æª”æ¡ˆå·²ç§»é™¤ `version` æ¬„ä½ï¼ˆv2 æ¨™æº–ï¼‰

---

## ğŸš€ å®Œæ•´å·¥ä½œæµç¨‹ç¯„ä¾‹

ä»¥ä¸‹æ˜¯å¾é›¶é–‹å§‹çš„å®Œæ•´æ¸¬è©¦æµç¨‹ï¼š

```bash
# ========================================
# éšæ®µ 1: ç’°å¢ƒæº–å‚™
# ========================================

# å•Ÿå‹• Docker ç’°å¢ƒ
cd /home/user/vllm_t/docker_setup
docker compose -f docker-compose.bench.yml up -d

# ç¢ºèªå®¹å™¨é‹è¡Œ
docker ps | grep vllm

# ========================================
# éšæ®µ 2: å•Ÿå‹• vLLM æœå‹™å™¨ï¼ˆçµ‚ç«¯ 1ï¼‰
# ========================================

# é€²å…¥æœå‹™å™¨å®¹å™¨
docker exec -it vllm-server bash

# å•Ÿå‹• vLLMï¼ˆ120B å¤§æ¨¡å‹ï¼‰
vllm serve openai/gpt-oss-120b \
  --tensor-parallel-size 4 \
  --gpu-memory-utilization 0.9 \
  --enforce-eager

# ç­‰å¾…æœå‹™å™¨å•Ÿå‹•å®Œæˆï¼ˆçœ‹åˆ° "Application startup complete"ï¼‰

# ========================================
# éšæ®µ 3: é‹è¡Œæ¸¬è©¦ï¼ˆçµ‚ç«¯ 2ï¼‰
# ========================================

# æª¢æŸ¥æœå‹™å™¨ç‹€æ…‹
curl http://localhost:8000/health

# é‹è¡Œ Production æ¸¬è©¦
docker exec vllm-bench-client bash /root/benchmark_tests/scripts/run_production_bench.sh

# ç­‰å¾…æ¸¬è©¦å®Œæˆï¼ˆç´„ 30-60 åˆ†é˜ï¼‰

# ========================================
# éšæ®µ 4: ç”Ÿæˆåœ–è¡¨ï¼ˆä¸»æ©Ÿç«¯ï¼‰
# ========================================

cd /home/user/vllm_t/benchmark_tests/plot_scripts

# ç”Ÿæˆæ‰€æœ‰åœ–è¡¨
./run_comprehensive_plot.sh
./run_normalized_plot.sh

# ========================================
# éšæ®µ 5: æŸ¥çœ‹çµæœ
# ========================================

# æŸ¥çœ‹æ¸¬è©¦çµæœ
ls -lh ../../bench_results/production/

# æŸ¥çœ‹åœ–è¡¨
ls -lh ../../output_plots/

# åˆ†æç‰¹å®šçµæœ
python3 -m json.tool ../../bench_results/production/input_10K_n5_*.json

# ========================================
# éšæ®µ 6: æ¸…ç†ï¼ˆå¯é¸ï¼‰
# ========================================

# åœæ­¢å®¹å™¨
cd /home/user/vllm_t/docker_setup
docker compose -f docker-compose.bench.yml stop

# æˆ–å®Œå…¨ç§»é™¤å®¹å™¨
docker compose -f docker-compose.bench.yml down
```

---

## ğŸ“ æ”¯æ´èˆ‡åé¥‹

å¦‚æœ‰å•é¡Œæˆ–å»ºè­°ï¼Œè«‹åƒè€ƒï¼š

1. **æ•…éšœæ’æŸ¥ç« ç¯€**: æŸ¥çœ‹ä¸Šæ–¹ã€ŒğŸ” æ•…éšœæ’æŸ¥ã€
2. **æ–‡æª”ç›®éŒ„**: æŸ¥çœ‹ `docs/` ç›®éŒ„ä¸­çš„è©³ç´°æ–‡æª”
3. **æ—¥èªŒåˆ†æ**: ä½¿ç”¨ `docker logs` æŸ¥çœ‹è©³ç´°éŒ¯èª¤ä¿¡æ¯

---

**ğŸ‰ é–‹å§‹æ‚¨çš„ vLLM æ€§èƒ½æ¸¬è©¦ä¹‹æ—…ï¼**
