# AMD å®˜æ–¹ vLLM å®¹å™¨ä½¿ç”¨æŒ‡å—

åŸºäº AMD ROCm å®˜æ–¹æ–‡æ¡£çš„ vLLM å®¹å™¨ä½¿ç”¨æ–¹æ³•ã€‚

å‚è€ƒ: [AMD ROCm vLLM æ–‡æ¡£](https://rocm.docs.amd.com/en/latest/how-to/rocm-for-ai/inference/benchmark-docker/vllm.html)

## ğŸ¯ å®˜æ–¹ Docker é•œåƒ

### æ¨èé•œåƒ

```bash
rocm/vllm:rocm7.0.0_vllm_0.10.2_20251006
```

**åŒ…å«å†…å®¹**:
- vLLM 0.11.0rc2.dev160 (å®é™…ç‰ˆæœ¬ï¼Œè€Œéæ ‡ç­¾çš„ 0.10.2)
- PyTorch 2.9.0a0 (é’ˆå¯¹ AMD GPU ä¼˜åŒ–)
- ROCm 7.0.0 (HIP 7.0.51831)
- é¢„é…ç½®çš„æ¨ç†ç¯å¢ƒ

**æ”¯æŒçš„ GPU**:
- AMD Instinct MI355X, MI350X, MI325X, MI300Xï¼ˆæ•°æ®ä¸­å¿ƒ GPUï¼‰
- AMD Radeon AI PRO R9700ï¼ˆgfx1201ï¼Œæœ¬é¡¹ç›®ä½¿ç”¨çš„ GPUï¼‰

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. æ‹‰å–é•œåƒ

```bash
docker pull rocm/vllm:rocm7.0.0_vllm_0.10.2_20251006
```

### 2. è¿è¡Œå®¹å™¨

```bash
# ä½¿ç”¨æˆ‘ä»¬çš„è„šæœ¬
cd /home/user/vllm_t
./host_scripts/enter_vllm_container.sh

# æˆ–è€…æ‰‹åŠ¨è¿è¡Œ
docker run -it --rm \
  --network=host \
  --group-add=video \
  --ipc=host \
  --cap-add=SYS_PTRACE \
  --security-opt seccomp=unconfined \
  --device /dev/kfd \
  --device /dev/dri \
  --shm-size=16g \
  -v /home/user/.cache/huggingface:/app/models \
  -v /home/user/vllm_t/container_scripts:/root/container_scripts \
  -e HF_HOME="/app/models" \
  -e PYTORCH_ALLOC_CONF="max_split_size_mb:512" \
  rocm/vllm:rocm7.0.0_vllm_0.10.2_20251006 \
  bash
```

### 3. åœ¨å®¹å™¨å†…æµ‹è¯•

```bash
# æ–¹æ³• A: è‡ªåŠ¨æµ‹è¯•ï¼ˆæ¨èï¼‰â­
python3 /root/container_scripts/test_vllm_auto.py facebook/opt-125m

# æ–¹æ³• B: åŸºç¡€æµ‹è¯•
python3 /root/container_scripts/test_vllm.py

# æ–¹æ³• C: äº¤äº’å¼ Python æµ‹è¯•
python3
```

**æ¨èä½¿ç”¨ test_vllm_auto.py**ï¼Œå®ƒä¼šè‡ªåŠ¨æ£€æµ‹æ¨¡å‹éœ€è¦çš„ dtype (float16/bfloat16)ã€‚

è¯¦ç»†æµ‹è¯•æ­¥éª¤è¯·å‚è€ƒ: [CONTAINER_TESTING.md](CONTAINER_TESTING.md)

## ğŸ“‹ å…³é”®é…ç½®å‚æ•°

### Docker è¿è¡Œå‚æ•°

| å‚æ•° | è¯´æ˜ | ä¸ºä»€ä¹ˆéœ€è¦ |
|------|------|-----------|
| `--network=host` | ä½¿ç”¨ä¸»æœºç½‘ç»œ | ç®€åŒ–ç½‘ç»œé…ç½® |
| `--device /dev/kfd` | KFD è®¾å¤‡ | ROCm GPU è®¿é—® |
| `--device /dev/dri` | DRI è®¾å¤‡ | GPU æ¸²æŸ“æ¥å£ |
| `--shm-size=16g` | å…±äº«å†…å­˜ | å¤§æ¨¡å‹éœ€è¦å¤§å†…å­˜ |
| `--group-add=video` | è§†é¢‘ç»„æƒé™ | GPU è®¿é—®æƒé™ |
| `--ipc=host` | IPC å‘½åç©ºé—´ | è¿›ç¨‹é—´é€šä¿¡ |

### ç¯å¢ƒå˜é‡

| å˜é‡ | å€¼ | è¯´æ˜ |
|------|-----|------|
| `HF_HOME` | `/app/models` | Hugging Face ç¼“å­˜ç›®å½• |
| `PYTORCH_ALLOC_CONF` | `max_split_size_mb:512` | å†…å­˜åˆ†é…ä¼˜åŒ– |
| `VLLM_LOGGING_LEVEL` | `INFO` æˆ– `DEBUG` | æ—¥å¿—çº§åˆ« |

### vLLM æ¨¡å‹å‚æ•°

| å‚æ•° | æ¨èå€¼ | è¯´æ˜ |
|------|--------|------|
| `tensor_parallel_size` | `1` | å• GPU ä½¿ç”¨ 1 |
| `dtype` | `float16` æˆ– `bfloat16` | è§ä¸‹æ–¹ dtype é€‰æ‹©æŒ‡å— |
| `max_model_len` | `256` - `2048` | æ ¹æ® GPU å†…å­˜è°ƒæ•´ |
| `gpu_memory_utilization` | `0.7` - `0.9` | GPU å†…å­˜ä½¿ç”¨ç‡ |
| `enforce_eager` | `True` | **å¿…é¡»ï¼** ç¦ç”¨ CUDA graphs |

#### dtype é€‰æ‹©æŒ‡å—

| æ¨¡å‹ç³»åˆ— | æ¨è dtype | åŸå›  |
|---------|-----------|------|
| OPT, GPT-2, Mistral | `float16` | æ ‡å‡†æ”¯æŒ |
| Llama 3 | `float16` æˆ– `bfloat16` | éƒ½æ”¯æŒ |
| **Gemma 2/3** | **`bfloat16`** | å¿…é¡»ï¼Œfloat16 ä¼šæŠ¥é”™ |
| Qwen 2 | `bfloat16` | æ¨è |
| DeepSeek R1 | `bfloat16` | æ¨è |

**æç¤º**: ä½¿ç”¨ `test_vllm_auto.py` ä¼šè‡ªåŠ¨é€‰æ‹©æ­£ç¡®çš„ dtypeï¼

## ğŸ¯ é’ˆå¯¹ gfx1201 çš„ä¼˜åŒ–

### å·²çŸ¥ä¿¡æ¯

- **GPU**: AMD Radeon AI PRO R9700ï¼ˆgfx1201ï¼ŒRDNA 3 Proï¼‰
- **ROCm ç‰ˆæœ¬**: 7.0.0ï¼ˆå®¹å™¨å†…ï¼‰
- **vLLM ç‰ˆæœ¬**: 0.10.2ï¼ˆæ¯” 0.11.0rc2 æ›´ç¨³å®šï¼‰

### æ¨èé…ç½®

```python
llm = LLM(
    model="facebook/opt-125m",  # æˆ–å…¶ä»–å°æ¨¡å‹
    tensor_parallel_size=1,      # å• GPU
    dtype="float16",             # åŠç²¾åº¦
    max_model_len=256,           # å¼€å§‹æ—¶ä½¿ç”¨å°å€¼
    gpu_memory_utilization=0.8,  # 80% GPU å†…å­˜
    enforce_eager=True,          # ç¦ç”¨ CUDA graphsï¼ˆgfx1201 æ”¯æŒå¯èƒ½ä¸å®Œæ•´ï¼‰
    disable_log_stats=True,      # å‡å°‘æ—¥å¿—è¾“å‡º
)
```

### å¦‚æœé‡åˆ°é—®é¢˜

1. **é™ä½å†…å­˜ä½¿ç”¨**:
   ```python
   gpu_memory_utilization=0.7  # ä» 0.8 é™åˆ° 0.7
   max_model_len=128           # å‡å°ä¸Šä¸‹æ–‡é•¿åº¦
   ```

2. **å¯ç”¨è¯¦ç»†æ—¥å¿—**:
   ```bash
   export VLLM_LOGGING_LEVEL=DEBUG
   ```

3. **æ£€æŸ¥ GPU å¯è§æ€§**:
   ```bash
   rocm-smi
   rocminfo | grep gfx
   ```

## ğŸ“Š æ€§èƒ½åŸºå‡†

### AMD å®˜æ–¹åŸºå‡†æµ‹è¯•

AMD æä¾›äº†é’ˆå¯¹ä»¥ä¸‹æ¨¡å‹çš„åŸºå‡†æµ‹è¯•:
- Llama 2 70B
- Llama 3.1 405B FP4
- Llama 3.3 70B FP8

### æœ¬é¡¹ç›®æµ‹è¯•ç›®æ ‡

å¯¹äº gfx1201ï¼ˆ16GB æ˜¾å­˜ï¼‰ï¼š

| æ¨¡å‹å¤§å° | é¢„æœŸæ€§èƒ½ | çŠ¶æ€ |
|----------|----------|------|
| å°æ¨¡å‹ï¼ˆ125M-1Bï¼‰ | 2000-3000 tokens/s | å¾…æµ‹è¯• |
| ä¸­æ¨¡å‹ï¼ˆ3B-7Bï¼‰ | 500-1500 tokens/s | å¾…æµ‹è¯• |
| å¤§æ¨¡å‹ï¼ˆ13B+ï¼‰ | å¯èƒ½å†…å­˜ä¸è¶³ | å¾…æµ‹è¯• |

## ğŸ”§ é«˜çº§ç”¨æ³•

### 1. è¿è¡Œ OpenAI å…¼å®¹çš„ API æœåŠ¡å™¨

```bash
python3 -m vllm.entrypoints.openai.api_server \
  --model facebook/opt-125m \
  --host 0.0.0.0 \
  --port 8000 \
  --dtype float16 \
  --max-model-len 256 \
  --gpu-memory-utilization 0.8
```

### 2. æ‰¹é‡æ¨ç†

```python
from vllm import LLM, SamplingParams

llm = LLM("facebook/opt-125m", enforce_eager=True)

prompts = [
    "The future of AI is",
    "Machine learning will",
    "Deep learning has",
]

outputs = llm.generate(prompts, SamplingParams(max_tokens=50))

for output in outputs:
    print(f"Prompt: {output.prompt}")
    print(f"Output: {output.outputs[0].text}\n")
```

### 3. æµ‹è¯•ä¸åŒæ¨¡å‹

```bash
# åœ¨å®¹å™¨å†…ï¼ˆè‡ªåŠ¨å¤„ç† dtypeï¼‰
python3 /root/container_scripts/test_vllm_auto.py facebook/opt-125m
python3 /root/container_scripts/test_vllm_auto.py Qwen/Qwen2-7B-Instruct
python3 /root/container_scripts/test_vllm_auto.py google/gemma-3-4b-it
```

æ›´å¤šæµ‹è¯•ç¤ºä¾‹è¯·å‚è€ƒ: [CONTAINER_TESTING.md](CONTAINER_TESTING.md)

## ğŸ› æ•…éšœæ’æŸ¥

### é—®é¢˜ 1: "Engine core proc died unexpectedly"

**å¯èƒ½åŸå› **:
- GPU å†…å­˜ä¸è¶³
- æ¨¡å‹ä¸ GPU æ¶æ„ä¸å…¼å®¹
- vLLM ç‰ˆæœ¬é—®é¢˜

**è§£å†³æ–¹æ¡ˆ**:
1. é™ä½ `gpu_memory_utilization` åˆ° 0.7
2. å‡å° `max_model_len`
3. ä½¿ç”¨æ›´å°çš„æ¨¡å‹æµ‹è¯•
4. å¯ç”¨ `enforce_eager=True`

### é—®é¢˜ 2: GPU ä¸å¯è§

**æ£€æŸ¥**:
```bash
# åœ¨å®¹å™¨å†…
rocm-smi
# åº”è¯¥èƒ½çœ‹åˆ° GPU ä¿¡æ¯

rocminfo | grep gfx
# åº”è¯¥æ˜¾ç¤º gfx1100 æˆ– gfx1201
```

**è§£å†³æ–¹æ¡ˆ**:
- ç¡®ä¿ Docker å‘½ä»¤åŒ…å« `--device /dev/kfd --device /dev/dri`
- æ£€æŸ¥ `--group-add=video` æ˜¯å¦å­˜åœ¨

### é—®é¢˜ 3: å†…å­˜ä¸è¶³

**ç—‡çŠ¶**:
```
RuntimeError: CUDA out of memory
```

**è§£å†³æ–¹æ¡ˆ**:
```python
# é€‰é¡¹ 1: é™ä½å†…å­˜ä½¿ç”¨
llm = LLM(
    model="facebook/opt-125m",
    gpu_memory_utilization=0.6,  # é™ä½åˆ° 60%
    max_model_len=128,            # å‡å°é•¿åº¦
)

# é€‰é¡¹ 2: ä½¿ç”¨æ›´å°çš„æ¨¡å‹
llm = LLM(model="facebook/opt-125m")  # è€Œä¸æ˜¯ opt-1.3b
```

## ğŸ“š AMD å®˜æ–¹èµ„æº

### æ–‡æ¡£é“¾æ¥

1. **vLLM æ¨ç†æ€§èƒ½æµ‹è¯•**:
   https://rocm.docs.amd.com/en/latest/how-to/rocm-for-ai/inference/benchmark-docker/vllm.html

2. **æ„å»º vLLM å®¹å™¨**:
   https://rocm.blogs.amd.com/software-tools-optimization/vllm-container/README.html

3. **MI300X æ€§èƒ½éªŒè¯**:
   https://rocm.docs.amd.com/en/docs-6.3.3/how-to/rocm-for-ai/inference/vllm-benchmark.html

4. **Radeon GPU ä¸Šä½¿ç”¨ vLLM**:
   https://rocm.docs.amd.com/projects/radeon/en/latest/docs/advanced/vllm/build-docker-image.html

### å®˜æ–¹ GitHub

- **vLLM é¡¹ç›®**: https://github.com/vllm-project/vllm
- **ROCm æ–‡æ¡£**: https://github.com/RadeonOpenCompute/ROCm

## ğŸ”„ ç‰ˆæœ¬å¯¹æ¯”

### vLLM 0.10.2 vs 0.11.0rc2

| ç‰¹æ€§ | 0.10.2ï¼ˆå½“å‰ï¼‰ | 0.11.0rc2ï¼ˆæµ‹è¯•å¤±è´¥ï¼‰ |
|------|----------------|----------------------|
| ç¨³å®šæ€§ | âœ… ç¨³å®šå‘å¸ƒç‰ˆ | âš ï¸ å€™é€‰ç‰ˆæœ¬ |
| gfx1201 æ”¯æŒ | â³ å¾…æµ‹è¯• | âŒ V1 å¼•æ“å´©æºƒ |
| ROCm ç‰ˆæœ¬ | 7.0.0 | 7.0.0 |
| æ¨èä½¿ç”¨ | âœ… æ¨è | âŒ ä¸æ¨è |

## ğŸ¯ ä¸‹ä¸€æ­¥

1. **æµ‹è¯•å®˜æ–¹å®¹å™¨**:
   ```bash
   ./host_scripts/enter_vllm_container.sh
   python3 /root/container_scripts/test_vllm.py
   ```

2. **å¦‚æœæˆåŠŸ**: è®°å½•æ€§èƒ½æ•°æ®ï¼Œå°è¯•æ›´å¤§çš„æ¨¡å‹

3. **å¦‚æœå¤±è´¥**: åˆ‡æ¢åˆ° PyTorch 2.8 å®¹å™¨æˆ–ä½¿ç”¨ Transformers

4. **æ€§èƒ½è°ƒä¼˜**: å‚è€ƒ AMD å®˜æ–¹åŸºå‡†æµ‹è¯•æ–¹æ³•

## ğŸ“ ç›¸å…³æ–‡æ¡£

- [CONTAINER_TESTING.md](CONTAINER_TESTING.md) - å®¹å™¨æµ‹è¯•æŒ‡å—
- [../troubleshooting/ERROR_ANALYSIS.md](../troubleshooting/ERROR_ANALYSIS.md) - é”™è¯¯åˆ†æ
- [PYTORCH_2.8_GUIDE.md](PYTORCH_2.8_GUIDE.md) - PyTorch 2.8 å¤‡é€‰æ–¹æ¡ˆ

---

**æ€»ç»“**: å®˜æ–¹ vLLM 0.10.2 å®¹å™¨æ˜¯æœ€ç¨³å®šçš„é€‰æ‹©ï¼Œä¼˜å…ˆæµ‹è¯•è¿™ä¸ªç‰ˆæœ¬ã€‚
