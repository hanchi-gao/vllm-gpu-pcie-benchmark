# å®¹å™¨æµ‹è¯•å¿«é€ŸæŒ‡å—

å®¹å™¨å†…æµ‹è¯•çš„å®é™…æ“ä½œæ­¥éª¤ã€‚

> **é…ç½®å‚æ•°è¯¦è§£**: å‚è§ [AMD_OFFICIAL_VLLM_GUIDE.md](AMD_OFFICIAL_VLLM_GUIDE.md)

## ğŸš€ 1. è¿›å…¥å®¹å™¨

```bash
cd /home/user/vllm_t

# å®˜æ–¹ vLLM å®¹å™¨ï¼ˆæ¨èï¼‰
./host_scripts/enter_vllm_container.sh
```

**å®¹å™¨è¯´æ˜**:
- é•œåƒ: `rocm/vllm:rocm7.0.0_vllm_0.10.2_20251006`
- vLLM ç‰ˆæœ¬: 0.11.0rc2.dev160 (å®é™…è¿è¡Œç‰ˆæœ¬)
- PyTorch: 2.9.0a0
- ROCm: 7.0.0 (HIP 7.0.51831)
- çŠ¶æ€: âœ… å·²éªŒè¯å¯ç”¨

**å…¶ä»–å®¹å™¨**:
- `enter_pytorch_container.sh` - PyTorch 2.8 + ROCm 7.1ï¼ˆå¤‡é€‰ï¼‰
- `enter_container.sh` - vLLM 0.11 å¼€å‘ç‰ˆ

## âœ… 2. æ£€æŸ¥ç¯å¢ƒ

```bash
# æŸ¥çœ‹ GPU
rocm-smi

# æ£€æŸ¥ PyTorch
python3 -c "import torch; print(f'PyTorch: {torch.__version__}, GPU: {torch.cuda.is_available()}')"

# æ£€æŸ¥ vLLM
python3 -c "import vllm; print(f'vLLM: {vllm.__version__}')"
```

## ğŸ¯ 3. è¿è¡Œæµ‹è¯•

### æ–¹æ³• A: è‡ªåŠ¨æµ‹è¯•ï¼ˆæ¨èï¼‰â­

**è‡ªåŠ¨å¤„ç† dtypeï¼Œé€‚åˆæ‰€æœ‰æ¨¡å‹**:

```bash
# æµ‹è¯•å°æ¨¡å‹ï¼ˆé»˜è®¤ä½¿ç”¨ 1 ä¸ª GPUï¼‰
python3 /root/container_scripts/test_vllm_auto.py facebook/opt-125m

# æµ‹è¯•ä½ çš„æœ¬åœ°æ¨¡å‹
python3 /root/container_scripts/test_vllm_auto.py Qwen/Qwen2-7B-Instruct
python3 /root/container_scripts/test_vllm_auto.py google/gemma-3-4b-it
python3 /root/container_scripts/test_vllm_auto.py meta-llama/Meta-Llama-3-8B-Instruct

# æŒ‡å®šä½¿ç”¨å¤šä¸ª GPUï¼ˆå¦‚æœä½ æœ‰å¤šä¸ª GPUï¼‰
python3 /root/container_scripts/test_vllm_auto.py Qwen/Qwen2-7B-Instruct --gpus 2

# è‡ªå®šä¹‰æ›´å¤šå‚æ•°
python3 /root/container_scripts/test_vllm_auto.py Qwen/Qwen2-7B-Instruct --gpus 2 --max-len 512 --gpu-util 0.9
```

**test_vllm_auto.py ç‰¹æ€§**:
- âœ… è‡ªåŠ¨æ£€æµ‹æ¨¡å‹éœ€è¦çš„ dtype (float16/bfloat16)
- âœ… è‡ªåŠ¨å¤„ç† Gemma 3 ç­‰ç‰¹æ®Šæ¨¡å‹
- âœ… æ˜¾ç¤ºæ€§èƒ½æ•°æ®ï¼ˆååé‡ã€åŠ è½½æ—¶é—´ï¼‰
- âœ… 5 æ­¥å®Œæ•´æµ‹è¯•æµç¨‹
- âœ… æ”¯æŒå¤š GPU å¹¶è¡Œæ¨ç†ï¼ˆ`--gpus N`ï¼‰
- âœ… å¯è‡ªå®šä¹‰åºåˆ—é•¿åº¦å’Œå†…å­˜ä½¿ç”¨ç‡

**å‘½ä»¤è¡Œå‚æ•°**:
| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `æ¨¡å‹åç§°` | `facebook/opt-125m` | ç¬¬ä¸€ä¸ªä½ç½®å‚æ•° |
| `--gpus N` | `1` | ä½¿ç”¨çš„ GPU æ•°é‡ï¼ˆtensor_parallel_sizeï¼‰ |
| `--max-len N` | `256` | æœ€å¤§åºåˆ—é•¿åº¦ |
| `--gpu-util F` | `0.8` | GPU å†…å­˜ä½¿ç”¨ç‡ï¼ˆ0.0-1.0ï¼‰ |

**ç¤ºä¾‹**:
```bash
# æŸ¥çœ‹å¸®åŠ©
python3 /root/container_scripts/test_vllm_auto.py --help

# å• GPUï¼Œé»˜è®¤å‚æ•°
python3 /root/container_scripts/test_vllm_auto.py Qwen/Qwen2-7B-Instruct

# åŒ GPUï¼Œæ›´é•¿åºåˆ—
python3 /root/container_scripts/test_vllm_auto.py Qwen/Qwen2-7B-Instruct --gpus 2 --max-len 1024

# å• GPUï¼Œä½å†…å­˜ä½¿ç”¨
python3 /root/container_scripts/test_vllm_auto.py google/gemma-3-4b-it --gpu-util 0.6 --max-len 128
```

---

### æ–¹æ³• A2: API æœåŠ¡å™¨æµ‹è¯• ğŸ†•

**test_vllm_serve.py - æµ‹è¯• OpenAI å…¼å®¹ API**:

```bash
# åŸºç¡€æµ‹è¯•ï¼ˆå¯åŠ¨æœåŠ¡å™¨ + æµ‹è¯• APIï¼‰
python3 /root/container_scripts/test_vllm_serve.py facebook/opt-125m

# æµ‹è¯•æœ¬åœ°æ¨¡å‹
python3 /root/container_scripts/test_vllm_serve.py Qwen/Qwen2-7B-Instruct

# ä½¿ç”¨ä¸åŒç«¯å£
python3 /root/container_scripts/test_vllm_serve.py Qwen/Qwen2-7B-Instruct --port 8080

# è‡ªå®šä¹‰å‚æ•°
python3 /root/container_scripts/test_vllm_serve.py Qwen/Qwen2-7B-Instruct \
  --gpus 2 \
  --max-len 1024 \
  --gpu-util 0.85
```

**test_vllm_serve.py ç‰¹æ€§**:
- âœ… è‡ªåŠ¨å¯åŠ¨ vLLM OpenAI API æœåŠ¡å™¨
- âœ… æµ‹è¯• `/v1/completions` ç«¯ç‚¹
- âœ… æµ‹è¯• `/v1/chat/completions` ç«¯ç‚¹
- âœ… æµ‹è¯•æµå¼è¾“å‡ºï¼ˆstreamingï¼‰
- âœ… æŸ¥è¯¢ `/v1/models` ç«¯ç‚¹
- âœ… æµ‹è¯•å®Œæˆåè‡ªåŠ¨å…³é—­æœåŠ¡å™¨

**å‘½ä»¤è¡Œå‚æ•°**:
| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `æ¨¡å‹åç§°` | `facebook/opt-125m` | ç¬¬ä¸€ä¸ªä½ç½®å‚æ•° |
| `--port N` | `8000` | API æœåŠ¡å™¨ç«¯å£ |
| `--gpus N` | `1` | ä½¿ç”¨çš„ GPU æ•°é‡ |
| `--max-len N` | `512` | æœ€å¤§åºåˆ—é•¿åº¦ |
| `--gpu-util F` | `0.3` | GPU å†…å­˜ä½¿ç”¨ç‡ï¼ˆAPI æœåŠ¡å™¨æ¨¡å¼é»˜è®¤è¾ƒä½ï¼‰|

**ä¸ test_vllm_auto.py çš„åŒºåˆ«**:
| ç‰¹æ€§ | test_vllm_auto.py | test_vllm_serve.py |
|------|-------------------|-------------------|
| æµ‹è¯•æ–¹å¼ | ç›´æ¥ Python API | OpenAI å…¼å®¹ HTTP API |
| é€‚ç”¨åœºæ™¯ | å¿«é€ŸåŠŸèƒ½æµ‹è¯• | API æœåŠ¡å™¨æµ‹è¯• |
| å¯åŠ¨æ—¶é—´ | å¿«ï¼ˆ~30ç§’ï¼‰ | è¾ƒæ…¢ï¼ˆ~2åˆ†é’Ÿï¼‰|
| æµ‹è¯•å†…å®¹ | æ¨ç†åŠŸèƒ½ | API ç«¯ç‚¹ã€æµå¼è¾“å‡º |
| ç”¨é€” | éªŒè¯æ¨¡å‹å¯ç”¨æ€§ | éªŒè¯ç”Ÿäº§ç¯å¢ƒéƒ¨ç½² |

---

### æ–¹æ³• B: åŸºç¡€æµ‹è¯•

```bash
# ä½¿ç”¨é»˜è®¤ float16
python3 /root/container_scripts/test_vllm.py facebook/opt-125m

# æŒ‡å®šæ¨¡å‹
python3 /root/container_scripts/test_vllm.py Qwen/Qwen2-7B-Instruct
```

**æ³¨æ„**: å¦‚æœé‡åˆ° "float16 not supported" é”™è¯¯ï¼Œä½¿ç”¨æ–¹æ³• A çš„ `test_vllm_auto.py`ã€‚

### æ–¹æ³• C: Transformers å¤‡é€‰

```bash
# å¦‚æœ vLLM æœ‰é—®é¢˜ï¼Œä½¿ç”¨ Transformers
python3 /root/container_scripts/test_transformers.py
python3 /root/container_scripts/test_transformers.py gpt2
```

## ğŸ“Š 4. æŸ¥çœ‹æœ¬åœ°æ¨¡å‹

```bash
# åˆ—å‡ºæ‰€æœ‰å·²ä¸‹è½½çš„æ¨¡å‹
ls /app/models/hub/

# ä½ æœ‰è¿™äº›æ¨¡å‹å¯ç”¨:
# - facebook/opt-125m
# - google/gemma-3-4b-it
# - Qwen/Qwen2-7B-Instruct
# - meta-llama/Meta-Llama-3-8B-Instruct
# - mistralai/Mistral-7B-Instruct-v0.3
# - google/gemma-2-9b-it
# - deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
# - deepseek-ai/DeepSeek-R1-Distill-Llama-70B
```

## ğŸ”§ 5. äº¤äº’å¼æµ‹è¯•

### åŸºç¡€ Python æµ‹è¯•

```bash
python3
```

```python
from vllm import LLM, SamplingParams

# åŠ è½½æ¨¡å‹
llm = LLM(
    "facebook/opt-125m",
    dtype="float16",
    enforce_eager=True,  # å¿…é¡»ï¼
    gpu_memory_utilization=0.8,
)

# è¿è¡Œæ¨ç†
outputs = llm.generate(
    ["Hello, my name is"],
    SamplingParams(temperature=0.8, max_tokens=20)
)

print(outputs[0].outputs[0].text)
```

### æµ‹è¯•ä¸­æ–‡æ¨¡å‹

```python
from vllm import LLM, SamplingParams

# ä½¿ç”¨ bfloat16 (Qwen æ¨è)
llm = LLM(
    "Qwen/Qwen2-7B-Instruct",
    dtype="bfloat16",
    enforce_eager=True,
    max_model_len=512,
)

outputs = llm.generate(
    ["ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±:"],
    SamplingParams(temperature=0.7, max_tokens=100)
)

print(outputs[0].outputs[0].text)
```

### æµ‹è¯• Gemma 3 (éœ€è¦ bfloat16)

```python
from vllm import LLM, SamplingParams

# Gemma 3 å¿…é¡»ä½¿ç”¨ bfloat16
llm = LLM(
    "google/gemma-3-4b-it",
    dtype="bfloat16",  # å¿…é¡»ï¼
    enforce_eager=True,
)

outputs = llm.generate(
    ["Explain quantum computing:"],
    SamplingParams(max_tokens=100)
)

print(outputs[0].outputs[0].text)
```

## ğŸ“ˆ 6. æ‰¹é‡æµ‹è¯•

### æµ‹è¯•å¤šä¸ªæ¨¡å‹

```bash
# åœ¨å®¹å™¨å†…
for model in facebook/opt-125m Qwen/Qwen2-7B-Instruct meta-llama/Meta-Llama-3-8B-Instruct; do
    echo "========================================="
    echo "æµ‹è¯•: $model"
    echo "========================================="
    python3 /root/container_scripts/test_vllm_auto.py "$model"
    echo ""
done
```

### æ€§èƒ½åŸºå‡†æµ‹è¯•

```python
from vllm import LLM, SamplingParams
import time

llm = LLM("facebook/opt-125m", enforce_eager=True)

# é¢„çƒ­
llm.generate(["warmup"], SamplingParams(max_tokens=10))

# æ‰¹é‡æµ‹è¯•
for batch_size in [1, 5, 10, 20]:
    prompts = ["Test"] * batch_size
    start = time.time()
    outputs = llm.generate(prompts, SamplingParams(max_tokens=20))
    elapsed = time.time() - start
    tokens = sum(len(o.outputs[0].token_ids) for o in outputs)
    print(f"æ‰¹é‡ {batch_size}: {tokens/elapsed:.1f} tokens/ç§’")
```

## ğŸ› 7. å¸¸è§é—®é¢˜å¿«é€Ÿè§£å†³

### Q: å‡ºç° "float16 not supported" é”™è¯¯ï¼Ÿ

```bash
# ä½¿ç”¨ test_vllm_auto.pyï¼Œä¼šè‡ªåŠ¨ä½¿ç”¨ bfloat16
python3 /root/container_scripts/test_vllm_auto.py google/gemma-3-4b-it
```

### Q: å†…å­˜ä¸è¶³ï¼Ÿ

```python
# é™ä½å†…å­˜ä½¿ç”¨
llm = LLM(
    model="...",
    gpu_memory_utilization=0.6,  # é™åˆ° 60%
    max_model_len=256,            # å‡å°ä¸Šä¸‹æ–‡
    enforce_eager=True,
)
```

### Q: æ¨¡å‹åŠ è½½å¾ˆæ…¢ï¼Ÿ

ç¬¬ä¸€æ¬¡è¿è¡Œéœ€è¦ç¼–è¯‘ GPU kernelsï¼ˆ~18ç§’ï¼‰ï¼Œä¹‹åä¼šå¿«å¾ˆå¤šï¼ˆ~2ç§’ï¼‰ã€‚è¿™æ˜¯æ­£å¸¸çš„ã€‚

### Q: å¦‚ä½•æŸ¥çœ‹è¯¦ç»†æ—¥å¿—ï¼Ÿ

```bash
export VLLM_LOGGING_LEVEL=DEBUG
python3 /root/container_scripts/test_vllm_auto.py <model>
```

### Q: å¦‚ä½•ç›‘æ§ GPUï¼Ÿ

```bash
# å®æ—¶ç›‘æ§ï¼ˆå¦å¼€ä¸€ä¸ªç»ˆç«¯ï¼‰
watch -n 1 rocm-smi
```

## ğŸ“‹ 8. æµ‹è¯•æ£€æŸ¥æ¸…å•

```bash
# 1. GPU å¯è§
rocm-smi && echo "âœ“ GPU å¯è§"

# 2. PyTorch å¯ç”¨
python3 -c "import torch; assert torch.cuda.is_available()" && echo "âœ“ PyTorch GPU"

# 3. vLLM å¯ç”¨
python3 -c "import vllm" && echo "âœ“ vLLM å¯ç”¨"

# 4. è¿è¡Œæµ‹è¯•
python3 /root/container_scripts/test_vllm_auto.py facebook/opt-125m && echo "âœ“ æµ‹è¯•é€šè¿‡"
```

## ğŸ¯ 9. æ¨èæµ‹è¯•æµç¨‹

### æ–°æ‰‹æµç¨‹

```bash
# 1. è¿›å…¥å®¹å™¨
./host_scripts/enter_vllm_container.sh

# 2. æ£€æŸ¥ç¯å¢ƒ
rocm-smi
python3 -c "import vllm; print(vllm.__version__)"

# 3. æµ‹è¯•å°æ¨¡å‹
python3 /root/container_scripts/test_vllm_auto.py facebook/opt-125m

# 4. æµ‹è¯•ä½ çš„æ¨¡å‹
python3 /root/container_scripts/test_vllm_auto.py Qwen/Qwen2-7B-Instruct
```

### é«˜çº§æµç¨‹

```bash
# 1. è¿›å…¥å®¹å™¨
./host_scripts/enter_vllm_container.sh

# 2. å¯åŠ¨ Python äº¤äº’å¼
python3
```

```python
# 3. è‡ªå®šä¹‰é…ç½®
from vllm import LLM, SamplingParams

llm = LLM(
    "Qwen/Qwen2-7B-Instruct",
    dtype="bfloat16",
    max_model_len=1024,           # æ›´é•¿çš„ä¸Šä¸‹æ–‡
    gpu_memory_utilization=0.9,   # æ›´é«˜çš„å†…å­˜ä½¿ç”¨
    enforce_eager=True,
)

# 4. æµ‹è¯•
outputs = llm.generate(
    ["ä½ çš„æç¤ºè¯"],
    SamplingParams(
        temperature=0.7,
        top_p=0.9,
        max_tokens=200
    )
)
```

## ğŸ“š ç›¸å…³æ–‡æ¡£

- **é…ç½®å‚æ•°è¯¦è§£**: [AMD_OFFICIAL_VLLM_GUIDE.md](AMD_OFFICIAL_VLLM_GUIDE.md)
- **ä¸»é¡¹ç›® README**: [../../README.md](../../README.md)
- **å¿«é€Ÿå¼€å§‹**: [../../QUICKSTART.md](../../QUICKSTART.md)

---

**æ€»ç»“**: ä½¿ç”¨ `test_vllm_auto.py` æ˜¯æœ€ç®€å•çš„æ–¹å¼ï¼Œä¼šè‡ªåŠ¨å¤„ç†æ‰€æœ‰ dtype é—®é¢˜ï¼
