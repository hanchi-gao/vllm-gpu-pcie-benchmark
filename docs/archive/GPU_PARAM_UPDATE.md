# test_vllm_auto.py GPU å‚æ•°æ›´æ–°

## ğŸ¯ æ›´æ–°å†…å®¹

ä¸º `test_vllm_auto.py` æ·»åŠ äº†å‘½ä»¤è¡Œå‚æ•°æ”¯æŒï¼Œå¯ä»¥çµæ´»æŒ‡å®š GPU æ•°é‡å’Œå…¶ä»–é…ç½®ã€‚

## âœ¨ æ–°å¢åŠŸèƒ½

### å‘½ä»¤è¡Œå‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `æ¨¡å‹åç§°` | `facebook/opt-125m` | ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ï¼Œæ¨¡å‹åç§°æˆ–è·¯å¾„ |
| `--gpus N` | `1` | ä½¿ç”¨çš„ GPU æ•°é‡ï¼ˆtensor_parallel_sizeï¼‰ |
| `--max-len N` | `256` | æœ€å¤§åºåˆ—é•¿åº¦ |
| `--gpu-util F` | `0.8` | GPU å†…å­˜ä½¿ç”¨ç‡ï¼ˆ0.0-1.0ï¼‰ |

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€ç”¨æ³•

```bash
# é»˜è®¤å‚æ•°ï¼ˆ1 GPUï¼Œ256 max_lenï¼Œ0.8 gpu_utilï¼‰
python3 /root/container_scripts/test_vllm_auto.py facebook/opt-125m
```

### æŒ‡å®š GPU æ•°é‡

```bash
# å• GPU
python3 /root/container_scripts/test_vllm_auto.py Qwen/Qwen2-7B-Instruct --gpus 1

# åŒ GPUï¼ˆéœ€è¦ä½ çš„æœºå™¨æœ‰ 2 ä¸ª GPUï¼‰
python3 /root/container_scripts/test_vllm_auto.py Qwen/Qwen2-7B-Instruct --gpus 2

# 4 ä¸ª GPU
python3 /root/container_scripts/test_vllm_auto.py meta-llama/Meta-Llama-3-8B-Instruct --gpus 4
```

### è‡ªå®šä¹‰é…ç½®

```bash
# æ›´é•¿çš„åºåˆ—é•¿åº¦
python3 /root/container_scripts/test_vllm_auto.py Qwen/Qwen2-7B-Instruct --max-len 1024

# é™ä½å†…å­˜ä½¿ç”¨ï¼ˆå¦‚æœé‡åˆ° OOMï¼‰
python3 /root/container_scripts/test_vllm_auto.py google/gemma-3-4b-it --gpu-util 0.6 --max-len 128

# ç»„åˆå‚æ•°
python3 /root/container_scripts/test_vllm_auto.py Qwen/Qwen2-7B-Instruct \
  --gpus 2 \
  --max-len 512 \
  --gpu-util 0.9
```

### æŸ¥çœ‹å¸®åŠ©

```bash
python3 /root/container_scripts/test_vllm_auto.py --help
```

è¾“å‡ºï¼š
```
usage: test_vllm_auto.py [-h] [--gpus GPUS] [--max-len MAX_LEN] [--gpu-util GPU_UTIL] [model]

vLLM æµ‹è¯•è„šæœ¬ - è‡ªåŠ¨æ£€æµ‹ dtype

positional arguments:
  model                æ¨¡å‹åç§°

options:
  -h, --help           show this help message and exit
  --gpus GPUS          ä½¿ç”¨çš„ GPU æ•°é‡ï¼ˆtensor_parallel_sizeï¼‰
  --max-len MAX_LEN    æœ€å¤§åºåˆ—é•¿åº¦
  --gpu-util GPU_UTIL  GPU å†…å­˜ä½¿ç”¨ç‡ (0.0-1.0)
```

## ğŸ” æ£€æŸ¥ä½ çš„ GPU æ•°é‡

åœ¨å®¹å™¨å†…è¿è¡Œï¼š

```bash
# æ–¹æ³• 1: ä½¿ç”¨ rocm-smi
rocm-smi

# æ–¹æ³• 2: ä½¿ç”¨ Python
python3 -c "import torch; print(f'å¯ç”¨ GPU æ•°é‡: {torch.cuda.device_count()}')"
```

## ğŸ’¡ ä½¿ç”¨å»ºè®®

### å• GPU ç³»ç»Ÿï¼ˆä½ çš„ R9700ï¼‰

```bash
# æ¨èé…ç½®
python3 /root/container_scripts/test_vllm_auto.py Qwen/Qwen2-7B-Instruct \
  --gpus 1 \
  --max-len 512 \
  --gpu-util 0.8
```

### å¤š GPU ç³»ç»Ÿ

```bash
# å¤§æ¨¡å‹ç”¨å¤š GPU
python3 /root/container_scripts/test_vllm_auto.py deepseek-ai/DeepSeek-R1-Distill-Llama-70B \
  --gpus 4 \
  --max-len 1024 \
  --gpu-util 0.9
```

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **GPU æ•°é‡å¿…é¡»æ˜¯ 2 çš„å¹‚æ¬¡**: 1, 2, 4, 8ï¼ˆvLLM é™åˆ¶ï¼‰
2. **ç¡®ä¿æœ‰è¶³å¤Ÿçš„ GPU**: å¦‚æœæŒ‡å®š `--gpus 2` ä½†åªæœ‰ 1 ä¸ª GPUï¼Œä¼šæŠ¥é”™
3. **å†…å­˜ä½¿ç”¨**:
   - å• GPU: `gpu_util=0.8` é€šå¸¸å®‰å…¨
   - å¤š GPU: å¯ä»¥æé«˜åˆ° `gpu_util=0.9`
4. **åºåˆ—é•¿åº¦**:
   - å°æ¨¡å‹ï¼ˆ< 1Bï¼‰: å¯ä»¥ç”¨ 512-2048
   - ä¸­æ¨¡å‹ï¼ˆ3-7Bï¼‰: 256-1024
   - å¤§æ¨¡å‹ï¼ˆ> 13Bï¼‰: 128-512

## ğŸ“Š æ€§èƒ½å¯¹æ¯”ç¤ºä¾‹

### å• GPU vs åŒ GPUï¼ˆç†è®ºï¼‰

| é…ç½® | æ¨¡å‹ | ååé‡ï¼ˆé¢„æœŸï¼‰ |
|------|------|---------------|
| 1 GPU | Qwen-7B | ~500 tokens/s |
| 2 GPU | Qwen-7B | ~900 tokens/s |
| 4 GPU | Llama-70B | ~300 tokens/s |

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [CONTAINER_TESTING.md](docs/guides/CONTAINER_TESTING.md) - å·²æ›´æ–°å‚æ•°è¯´æ˜
- [AMD_OFFICIAL_VLLM_GUIDE.md](docs/guides/AMD_OFFICIAL_VLLM_GUIDE.md) - tensor_parallel_size è¯¦è§£

## ğŸ‰ æ€»ç»“

ç°åœ¨ä½ å¯ä»¥ï¼š
- âœ… çµæ´»æŒ‡å®š GPU æ•°é‡
- âœ… è‡ªå®šä¹‰åºåˆ—é•¿åº¦å’Œå†…å­˜ä½¿ç”¨
- âœ… ä½¿ç”¨ `--help` æŸ¥çœ‹æ‰€æœ‰é€‰é¡¹
- âœ… é€‚åº”ä¸åŒçš„ç¡¬ä»¶é…ç½®

**æœ€é‡è¦çš„æ˜¯**: `test_vllm_auto.py` ä»ç„¶ä¼šè‡ªåŠ¨æ£€æµ‹å’Œä½¿ç”¨æ­£ç¡®çš„ dtypeï¼
