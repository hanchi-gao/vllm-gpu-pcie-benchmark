# Docker Compose æ‰‹åŠ¨åŸºå‡†æµ‹è¯•æŒ‡å—

ä¸¤ä¸ªç‹¬ç«‹å®¹å™¨ï¼Œå¯ä»¥æ‰‹åŠ¨è¿›å…¥æ‰§è¡Œå‘½ä»¤å’Œä¿®æ”¹å‚æ•°ã€‚

## ğŸ—ï¸ æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    vllm-bench-client (å®¢æˆ·ç«¯å®¹å™¨)        â”‚
â”‚    - æ‰‹åŠ¨è¿›å…¥                            â”‚
â”‚    - è¿è¡Œ vllm bench serve              â”‚
â”‚    - ä¿å­˜ç»“æœåˆ° /root/bench_results     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚ HTTP (vllm-network)
              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    vllm-server (æœåŠ¡å™¨å®¹å™¨)              â”‚
â”‚    - æ‰‹åŠ¨è¿›å…¥                            â”‚
â”‚    - è¿è¡Œ vllm serve                    â”‚
â”‚    - ç«¯å£: 8000                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å¯åŠ¨ä¸¤ä¸ªå®¹å™¨

```bash
# å¯åŠ¨å®¹å™¨ï¼ˆåå°è¿è¡Œï¼‰
docker compose -f docker-compose.bench.yml up -d

# æŸ¥çœ‹çŠ¶æ€
docker compose -f docker-compose.bench.yml ps
```

è¾“å‡ºï¼š
```
NAME                IMAGE                                       STATUS
vllm-server         rocm/vllm:rocm7.0.0_vllm_0.10.2_20251006   Up
vllm-bench-client   rocm/vllm:rocm7.0.0_vllm_0.10.2_20251006   Up
```

---

## ğŸ“‹ ä½¿ç”¨æµç¨‹

### ç»ˆç«¯ 1: å¯åŠ¨ vLLM æœåŠ¡å™¨

```bash
# è¿›å…¥æœåŠ¡å™¨å®¹å™¨
docker exec -it vllm-server bash

# å¯åŠ¨ vLLM æœåŠ¡ï¼ˆæ ¹æ®éœ€è¦ä¿®æ”¹å‚æ•°ï¼‰
vllm serve openai/gpt-oss-120b \
  --host 0.0.0.0 \
  --port 8000 \
  --dtype bfloat16 \
  --tensor-parallel-size 4 \
  --max-model-len 1024 \
  --gpu-memory-utilization 0.8 \
  --enforce-eager
```

**æˆ–è€…ä½¿ç”¨ Python æ¨¡å—æ–¹å¼**ï¼š
```bash
python3 -m vllm.entrypoints.openai.api_server \
  --model openai/gpt-oss-120b \
  --host 0.0.0.0 \
  --port 8000 \
  --dtype bfloat16 \
  --tensor-parallel-size 4 \
  --max-model-len 1024 \
  --gpu-memory-utilization 0.8 \
  --enforce-eager \
  --disable-log-stats
```

### ç»ˆç«¯ 2: è¿è¡ŒåŸºå‡†æµ‹è¯•

```bash
# è¿›å…¥å®¢æˆ·ç«¯å®¹å™¨
docker exec -it vllm-bench-client bash

# è¿è¡ŒåŸºå‡†æµ‹è¯•
vllm bench serve \
  --backend openai \
  --base-url http://vllm-server:8000 \
  --endpoint /v1/completions \
  --model openai/gpt-oss-120b \
  --num-prompts 32 \
  --max-concurrency 32 \
  --dataset-name random \
  --save-result
```

---

## ğŸ¯ å®Œæ•´æµ‹è¯•ç¤ºä¾‹

### ç¤ºä¾‹ 1: æµ‹è¯•ä¸åŒå¹¶å‘æ•°

**ç»ˆç«¯ 1 - æœåŠ¡å™¨**ï¼š
```bash
docker exec -it vllm-server bash

# å¯åŠ¨æœåŠ¡å™¨ï¼ˆä¿æŒè¿è¡Œï¼‰
vllm serve openai/gpt-oss-120b \
  --tensor-parallel-size 4 \
  --enforce-eager
```

**ç»ˆç«¯ 2 - å®¢æˆ·ç«¯**ï¼š
```bash
docker exec -it vllm-bench-client bash

# æµ‹è¯• 1 å¹¶å‘
vllm bench serve \
  --backend openai \
  --base-url http://vllm-server:8000 \
  --endpoint /v1/completions \
  --model openai/gpt-oss-120b \
  --num-prompts 20 \
  --max-concurrency 1

# æµ‹è¯• 8 å¹¶å‘
vllm bench serve \
  --backend openai \
  --base-url http://vllm-server:8000 \
  --endpoint /v1/completions \
  --model openai/gpt-oss-120b \
  --num-prompts 20 \
  --max-concurrency 8

# æµ‹è¯• 16 å¹¶å‘
vllm bench serve \
  --backend openai \
  --base-url http://vllm-server:8000 \
  --endpoint /v1/completions \
  --model openai/gpt-oss-120b \
  --num-prompts 20 \
  --max-concurrency 16

# æµ‹è¯• 32 å¹¶å‘
vllm bench serve \
  --backend openai \
  --base-url http://vllm-server:8000 \
  --endpoint /v1/completions \
  --model openai/gpt-oss-120b \
  --num-prompts 32 \
  --max-concurrency 32 \
  --save-result
```

---

### ç¤ºä¾‹ 2: ä½¿ç”¨è„šæœ¬è‡ªåŠ¨åŒ–æµ‹è¯•

åœ¨å®¢æˆ·ç«¯å®¹å™¨ä¸­åˆ›å»ºæµ‹è¯•è„šæœ¬ï¼š

```bash
docker exec -it vllm-bench-client bash

# åˆ›å»ºæµ‹è¯•è„šæœ¬
cat > /root/run_bench_tests.sh << 'EOF'
#!/bin/bash

SERVER_URL="http://vllm-server:8000"
MODEL="openai/gpt-oss-120b"
NUM_PROMPTS=20

echo "Testing different concurrency levels..."

for concurrency in 1 2 4 8 16 32; do
    echo "=========================================="
    echo "Testing concurrency: $concurrency"
    echo "=========================================="

    vllm bench serve \
        --backend openai \
        --base-url $SERVER_URL \
        --endpoint /v1/completions \
        --model $MODEL \
        --num-prompts $NUM_PROMPTS \
        --max-concurrency $concurrency \
        --save-result

    # ä¿å­˜ç»“æœ
    if [ -f "results.json" ]; then
        mv results.json /root/bench_results/results_${concurrency}.json
    fi

    echo
    sleep 5
done

echo "All tests completed!"
echo "Results saved to /root/bench_results/"
EOF

chmod +x /root/run_bench_tests.sh

# è¿è¡Œæµ‹è¯•
/root/run_bench_tests.sh
```

---

## ğŸ“Š ä¿å­˜å’ŒæŸ¥çœ‹ç»“æœ

### æ‰‹åŠ¨ä¿å­˜ç»“æœ

åœ¨å®¢æˆ·ç«¯å®¹å™¨ä¸­ï¼š
```bash
# vllm bench ä¼šç”Ÿæˆ results.json
# å¤åˆ¶åˆ°æŒä¹…åŒ–ç›®å½•
cp results.json /root/bench_results/gpt-oss-120b_4gpu_32concurrency.json

# æˆ–è€…å¸¦æ—¶é—´æˆ³
cp results.json /root/bench_results/test_$(date +%Y%m%d_%H%M%S).json
```

### åœ¨ä¸»æœºä¸ŠæŸ¥çœ‹ç»“æœ

```bash
# æŸ¥çœ‹æ‰€æœ‰ç»“æœ
ls -lh bench_results/

# æŸ¥çœ‹ç‰¹å®šç»“æœ
cat bench_results/gpt-oss-120b_4gpu_32concurrency.json

# ç¾åŒ–è¾“å‡º
python3 -m json.tool bench_results/gpt-oss-120b_4gpu_32concurrency.json
```

---

## ğŸ”§ å¸¸ç”¨æ“ä½œ

### å¯åŠ¨å’Œåœæ­¢å®¹å™¨

```bash
# å¯åŠ¨å®¹å™¨
docker compose -f docker-compose.bench.yml up -d

# åœæ­¢å®¹å™¨
docker compose -f docker-compose.bench.yml stop

# é‡å¯å®¹å™¨
docker compose -f docker-compose.bench.yml restart

# å®Œå…¨åˆ é™¤å®¹å™¨
docker compose -f docker-compose.bench.yml down
```

### è¿›å…¥å®¹å™¨

```bash
# è¿›å…¥æœåŠ¡å™¨å®¹å™¨
docker exec -it vllm-server bash

# è¿›å…¥å®¢æˆ·ç«¯å®¹å™¨
docker exec -it vllm-bench-client bash
```

### æŸ¥çœ‹æ—¥å¿—

```bash
# æŸ¥çœ‹æœåŠ¡å™¨å®¹å™¨æ—¥å¿—
docker logs -f vllm-server

# æŸ¥çœ‹å®¢æˆ·ç«¯å®¹å™¨æ—¥å¿—
docker logs -f vllm-bench-client
```

### æ£€æŸ¥ç½‘ç»œè¿æ¥

åœ¨å®¢æˆ·ç«¯å®¹å™¨ä¸­ï¼š
```bash
# æ£€æŸ¥æœåŠ¡å™¨å¥åº·çŠ¶æ€
curl http://vllm-server:8000/health

# æµ‹è¯• API ç«¯ç‚¹
curl http://vllm-server:8000/v1/models
```

---

## ğŸ“ vllm serve å¸¸ç”¨å‚æ•°

### åŸºç¡€å‚æ•°
```bash
vllm serve <model_name> \
  --host 0.0.0.0 \              # ç›‘å¬æ‰€æœ‰ç½‘ç»œæ¥å£
  --port 8000 \                 # ç«¯å£å·
  --dtype bfloat16 \            # æ•°æ®ç±»å‹
  --tensor-parallel-size 4 \    # GPU æ•°é‡
  --max-model-len 1024 \        # æœ€å¤§åºåˆ—é•¿åº¦
  --gpu-memory-utilization 0.8  # GPU å†…å­˜ä½¿ç”¨ç‡
```

### å¿…éœ€å‚æ•°ï¼ˆAMD GPUï¼‰
```bash
--enforce-eager               # ç¦ç”¨ CUDA graphsï¼ˆgfx1201 å¿…éœ€ï¼‰
```

### å¯é€‰ä¼˜åŒ–å‚æ•°
```bash
--disable-log-stats          # ç¦ç”¨ç»Ÿè®¡æ—¥å¿—
--max-num-seqs 256           # æœ€å¤§æ‰¹å¤„ç†åºåˆ—æ•°
--max-num-batched-tokens 8192  # æœ€å¤§æ‰¹å¤„ç† token æ•°
```

---

## ğŸ“ vllm bench serve å¸¸ç”¨å‚æ•°

### åŸºç¡€å‚æ•°
```bash
vllm bench serve \
  --backend openai \                    # åç«¯ç±»å‹
  --base-url http://vllm-server:8000 \  # æœåŠ¡å™¨åœ°å€
  --endpoint /v1/completions \          # API ç«¯ç‚¹
  --model <model_name> \                # æ¨¡å‹åç§°
  --num-prompts 32 \                    # è¯·æ±‚æ€»æ•°
  --max-concurrency 32                  # æœ€å¤§å¹¶å‘æ•°
```

### æ•°æ®é›†å‚æ•°
```bash
--dataset-name random \       # ä½¿ç”¨éšæœºæ•°æ®é›†
--random-input-len 128 \      # éšæœºè¾“å…¥é•¿åº¦
--random-output-len 128       # éšæœºè¾“å‡ºé•¿åº¦

# æˆ–ä½¿ç”¨çœŸå®æ•°æ®é›†
--dataset-name sharegpt \     # ä½¿ç”¨ ShareGPT æ•°æ®é›†
--dataset-path ./sharegpt.json
```

### è¾“å‡ºå‚æ•°
```bash
--save-result                 # ä¿å­˜ç»“æœåˆ° results.json
--result-dir /path/to/dir     # æŒ‡å®šç»“æœç›®å½•
--result-filename custom.json # è‡ªå®šä¹‰ç»“æœæ–‡ä»¶å
```

---

## ğŸ†š vllm serve vs python3 -m vllm.entrypoints.openai.api_server

ä¸¤ç§æ–¹å¼åŠŸèƒ½ç›¸åŒï¼Œé€‰æ‹©ä»»ä¸€å³å¯ï¼š

### æ–¹å¼ 1: vllm serveï¼ˆç®€æ´ï¼‰
```bash
vllm serve openai/gpt-oss-120b \
  --tensor-parallel-size 4 \
  --enforce-eager
```

### æ–¹å¼ 2: Python æ¨¡å—ï¼ˆè¯¦ç»†ï¼‰
```bash
python3 -m vllm.entrypoints.openai.api_server \
  --model openai/gpt-oss-120b \
  --host 0.0.0.0 \
  --port 8000 \
  --tensor-parallel-size 4 \
  --enforce-eager
```

---

## ğŸ’¡ æç¤ºå’ŒæŠ€å·§

### 1. åå°è¿è¡ŒæœåŠ¡å™¨
```bash
# åœ¨æœåŠ¡å™¨å®¹å™¨ä¸­
nohup vllm serve openai/gpt-oss-120b \
  --tensor-parallel-size 4 \
  --enforce-eager \
  > /root/bench_results/server.log 2>&1 &

# æ£€æŸ¥è¿›ç¨‹
ps aux | grep vllm
```

### 2. ç›‘æ§ GPU ä½¿ç”¨
```bash
# åœ¨æœåŠ¡å™¨å®¹å™¨ä¸­
watch -n 1 rocm-smi
```

### 3. æ‰¹é‡æµ‹è¯•è„šæœ¬
```bash
# åœ¨å®¢æˆ·ç«¯å®¹å™¨ä¸­
for i in 1 2 4 8 16 32; do
  echo "Testing concurrency: $i"
  vllm bench serve \
    --backend openai \
    --base-url http://vllm-server:8000 \
    --endpoint /v1/completions \
    --model openai/gpt-oss-120b \
    --num-prompts 20 \
    --max-concurrency $i \
    --save-result

  mv results.json /root/bench_results/results_${i}.json
  sleep 5
done
```

### 4. æ¸…ç†å’Œé‡å¯
```bash
# ä¸»æœºä¸Š
docker compose -f docker-compose.bench.yml restart

# æˆ–è€…å®Œå…¨é‡å»º
docker compose -f docker-compose.bench.yml down
docker compose -f docker-compose.bench.yml up -d
```

---

## ğŸ‰ æ€»ç»“

ä½¿ç”¨ Docker Compose æ‰‹åŠ¨æ¨¡å¼çš„ä¼˜åŠ¿ï¼š

âœ… **å®Œå…¨æ§åˆ¶**: æ‰‹åŠ¨å¯åŠ¨æœåŠ¡å™¨å’Œæµ‹è¯•
âœ… **çµæ´»è°ƒæ•´**: éšæ—¶ä¿®æ”¹å‚æ•°å’Œé…ç½®
âœ… **ç‹¬ç«‹å®¹å™¨**: æœåŠ¡å™¨å’Œå®¢æˆ·ç«¯å®Œå…¨éš”ç¦»
âœ… **æŒä¹…åŒ–ç»“æœ**: ç»“æœä¿å­˜åˆ°ä¸»æœº `bench_results/` ç›®å½•
âœ… **ç½‘ç»œäº’é€š**: å®¹å™¨é—´é€šè¿‡ `vllm-network` é€šä¿¡
âœ… **å¤šæ¬¡æµ‹è¯•**: å¯ä»¥è¿è¡Œå¤šè½®æµ‹è¯•è€Œä¸é‡å¯æœåŠ¡å™¨

**ç°åœ¨å¯ä»¥å®Œå…¨æ‰‹åŠ¨æ§åˆ¶åŸºå‡†æµ‹è¯•æµç¨‹äº†ï¼** ğŸš€
