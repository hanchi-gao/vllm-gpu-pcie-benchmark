# AMD GPU ç¡¬é«”æ¸¬è©¦æ‰‹å†Š

> å¿«é€Ÿé©—è­‰ AMD ç¡¬é«”æ˜¯å¦èƒ½æ­£å¸¸é‹è¡Œ AI æ¨¡å‹

**å°ˆæ¡ˆ GitHub**: [https://github.com/hanchi-gao/vllm_t](https://github.com/hanchi-gao/vllm_t)

---

## ğŸ“‹ æ‰‹å†Šç›®çš„

ç•¶æ”¶åˆ°æ–°çš„ AMD GPU ç¡¬é«”æ™‚ï¼Œä½¿ç”¨æœ¬æ‰‹å†Šå¿«é€Ÿé©—è­‰ï¼š

âœ… **ç¡¬é«”æ˜¯å¦æ­£å¸¸å·¥ä½œ**
âœ… **èƒ½å¦æˆåŠŸè¼‰å…¥ AI æ¨¡å‹**
âœ… **æ¨¡å‹æ¨è«–æ˜¯å¦ç©©å®š**
âœ… **åŸºæœ¬æ€§èƒ½æ˜¯å¦ç¬¦åˆé æœŸ**

---

## ğŸ¯ æ¸¬è©¦æµç¨‹ç¸½è¦½

```
æ”¶åˆ°æ–°ç¡¬é«” â†’ ç’°å¢ƒè¨­ç½® â†’ æ¸¬è©¦æ¨¡å‹è¼‰å…¥ â†’ åŸ·è¡Œå£“åŠ›æ¸¬è©¦ â†’ æ”¶é›†çµæœ â†’ åˆ¤å®šåˆæ ¼èˆ‡å¦
   (5åˆ†é˜)     (10åˆ†é˜)      (5-10åˆ†é˜)       (1-3å°æ™‚)      (æŸ¥çœ‹æª”æ¡ˆ)   (æ ¹æ“šæ•¸æ“š)
```

---

## ğŸ“ æ¸¬è©¦å‰æº–å‚™

### ç¡¬é«”è³‡è¨Šè¨˜éŒ„

**åœ¨é–‹å§‹æ¸¬è©¦å‰ï¼Œè«‹è¨˜éŒ„ä»¥ä¸‹ç¡¬é«”è³‡è¨Š**ï¼š

```bash
# æŸ¥çœ‹ GPU è³‡è¨Š
amd-smi

# æŸ¥çœ‹è©³ç´°è¦æ ¼
rocminfo | grep -E "Name|Marketing Name|gfx"
```

**è¨˜éŒ„ç¯„ä¾‹**ï¼š
- GPU å‹è™Ÿ: AMD Radeon AI PRO R9700
- æ¶æ§‹ä»£è™Ÿ: gfx1201
- GPU æ•¸é‡: 4
- VRAM: æ¯å¡ 32GB

---

## ğŸš€ å¿«é€Ÿæ¸¬è©¦æ­¥é©Ÿ

### æ­¥é©Ÿ 1: å•Ÿå‹•æ¸¬è©¦ç’°å¢ƒ (5 åˆ†é˜)

```bash
# é€²å…¥å°ˆæ¡ˆç›®éŒ„
cd /path/to/vllm_t/docker_setup

# å•Ÿå‹• Docker å®¹å™¨
docker compose -f docker-compose.bench.yml up -d

# ç¢ºèªå®¹å™¨é‹è¡Œ
docker ps | grep vllm
```

**é æœŸçµæœ**ï¼šçœ‹åˆ° `vllm-server` å’Œ `vllm-bench-client` å…©å€‹å®¹å™¨åœ¨é‹è¡Œ

---

### æ­¥é©Ÿ 2: å¿«é€Ÿæ¨¡å‹è¼‰å…¥æ¸¬è©¦ (5-10 åˆ†é˜)

**ç›®çš„**: é©—è­‰ç¡¬é«”èƒ½å¦æˆåŠŸè¼‰å…¥æ¨¡å‹

#### æ¸¬è©¦å°å‹æ¨¡å‹ (å»ºè­°å…ˆæ¸¬è©¦)

**é–‹å•Ÿçµ‚ç«¯ 1**ï¼Œé€²å…¥æœå‹™å™¨å®¹å™¨ï¼š

```bash
docker exec -it vllm-server bash
```

**æ¸¬è©¦ Gemma 3 4B** (è¼•é‡ç´šæ¨¡å‹ï¼Œé©åˆå¿«é€Ÿé©—è­‰)ï¼š

```bash
vllm serve google/gemma-3-4b-it \
  --tensor-parallel-size 1 \
  --gpu-memory-utilization 0.8 \
  --enforce-eager
```

**æˆåŠŸæ¨™èªŒ**ï¼š
- çœ‹åˆ° `Loading model weights completed`
- çœ‹åˆ° `Application startup complete`
- æ²’æœ‰éŒ¯èª¤è¨Šæ¯

**å¦‚æœæˆåŠŸ**ï¼šæŒ‰ `Ctrl+C` åœæ­¢ï¼Œç¹¼çºŒæ¸¬è©¦å¤§å‹æ¨¡å‹

#### æ¸¬è©¦å¤§å‹æ¨¡å‹ (å¦‚æœæœ‰å¤šå¼µ GPU)

```bash
vllm serve openai/gpt-oss-120b \
  --tensor-parallel-size 4 \
  --gpu-memory-utilization 0.9 \
  --enforce-eager
```

**æˆåŠŸæ¨™èªŒ**ï¼š
- æ¨¡å‹æˆåŠŸè¼‰å…¥
- æ²’æœ‰ GPU è¨˜æ†¶é«”ä¸è¶³éŒ¯èª¤
- æ²’æœ‰ ROCm é©…å‹•éŒ¯èª¤

---

### æ­¥é©Ÿ 3: åŸ·è¡Œå£“åŠ›æ¸¬è©¦ (1-3 å°æ™‚)

**ç›®çš„**: ç¢ºèªç¡¬é«”åœ¨æŒçºŒè² è¼‰ä¸‹ç©©å®šé‹è¡Œ

ä¿æŒçµ‚ç«¯ 1 çš„ vLLM æœå‹™å™¨é‹è¡Œï¼Œ**é–‹å•Ÿçµ‚ç«¯ 2**ï¼š

#### é¸é … A: å¿«é€Ÿé©—è­‰æ¸¬è©¦ (å»ºè­°)

ä½¿ç”¨å°å‹æ¨¡å‹ï¼Œæ¸¬è©¦æ™‚é–“è¼ƒçŸ­ï¼š

```bash
docker exec vllm-bench-client bash -c \
  "cd /root && /root/benchmark_tests/scripts/run_scaling_bench_200.sh \
  --model google/gemma-3-4b-it"
```

**é ä¼°æ™‚é–“**: 30-60 åˆ†é˜

#### é¸é … B: å®Œæ•´å£“åŠ›æ¸¬è©¦

ä½¿ç”¨å¤§å‹æ¨¡å‹ï¼Œæ¸¬è©¦ç¡¬é«”æ¥µé™ï¼š

```bash
docker exec vllm-bench-client bash /root/benchmark_tests/scripts/run_scaling_bench_200.sh
```

**é ä¼°æ™‚é–“**: 1-3 å°æ™‚

---

### æ­¥é©Ÿ 4: ç›£æ§æ¸¬è©¦éç¨‹

**åœ¨æ¸¬è©¦éç¨‹ä¸­ï¼Œé–‹å•Ÿçµ‚ç«¯ 3 ç›£æ§ç¡¬é«”ç‹€æ…‹**ï¼š

```bash
# å³æ™‚ç›£æ§ GPU ä½¿ç”¨ç‡
watch -n 1 amd-smi

# æˆ–åœ¨ Docker å…§ç›£æ§
docker exec vllm-server watch -n 1 amd-smi
```

**è§€å¯Ÿé‡é»**ï¼š
- GPU ä½¿ç”¨ç‡æ˜¯å¦æ­£å¸¸ (æ‡‰è©²æ¥è¿‘ 100%)
- æº«åº¦æ˜¯å¦ç©©å®š (ä¸æ‡‰æŒçºŒä¸Šå‡)
- è¨˜æ†¶é«”ä½¿ç”¨æ˜¯å¦ç©©å®š
- æ˜¯å¦æœ‰éŒ¯èª¤è¨Šæ¯

---

### æ­¥é©Ÿ 5: æ”¶é›†æ¸¬è©¦çµæœ

æ¸¬è©¦å®Œæˆå¾Œï¼Œçµæœæœƒå„²å­˜åœ¨ï¼š

```bash
/home/user/vllm_t/bench_results/scaling/{æ¨¡å‹åç¨±}/
```

#### æŸ¥çœ‹çµæœæª”æ¡ˆ

```bash
# åˆ—å‡ºæ‰€æœ‰çµæœæª”æ¡ˆ
ls -lh bench_results/scaling/*/

# æŸ¥çœ‹ç‰¹å®šæ¨¡å‹çš„çµæœ
ls -lh bench_results/scaling/gemma-3-4b-it/

# æŸ¥çœ‹å–®ä¸€çµæœæª”æ¡ˆ
cat bench_results/scaling/gemma-3-4b-it/gemma-3-4b-it_scale_n10_*.json | python3 -m json.tool
```

---

## âœ… åˆ¤å®šæ¨™æº–

### ç¡¬é«”åˆæ ¼æ¨™æº–

**âœ… é€šéæ¸¬è©¦** (ç¡¬é«”æ­£å¸¸)ï¼š
- [ ] æ¨¡å‹æˆåŠŸè¼‰å…¥ï¼Œç„¡éŒ¯èª¤è¨Šæ¯
- [ ] å®Œæˆæ‰€æœ‰ 200 å€‹æ¸¬è©¦é»
- [ ] ç”Ÿæˆ 200 å€‹ JSON çµæœæª”æ¡ˆ
- [ ] æ¸¬è©¦éç¨‹ä¸­ç„¡å´©æ½°æˆ–é‡å•Ÿ
- [ ] GPU æº«åº¦ç©©å®šï¼Œæœªéç†±
- [ ] æ€§èƒ½æ•¸æ“šåˆç† (throughput > 0, latency æœ‰æ•¸å€¼)

**âŒ æ¸¬è©¦å¤±æ•—** (ç¡¬é«”å¯èƒ½æœ‰å•é¡Œ)ï¼š
- [ ] æ¨¡å‹è¼‰å…¥å¤±æ•—
- [ ] æ¸¬è©¦ä¸­é€”å´©æ½°
- [ ] GPU éç†±ä¿è­·è§¸ç™¼
- [ ] å¤§é‡éŒ¯èª¤è¨Šæ¯
- [ ] çµæœæª”æ¡ˆæ•¸é‡ä¸è¶³ 200 å€‹
- [ ] æ€§èƒ½ç•°å¸¸ä½è½

---

## ğŸ“Š é—œéµæŒ‡æ¨™èªªæ˜

æ¯å€‹æ¸¬è©¦çµæœåŒ…å«ä»¥ä¸‹æŒ‡æ¨™ï¼Œç”¨æ–¼åˆ¤æ–·ç¡¬é«”æ€§èƒ½ï¼š

| æŒ‡æ¨™ | èªªæ˜ | åƒè€ƒå€¼ |
|------|------|--------|
| **throughput** | æ¯ç§’è™•ç†è«‹æ±‚æ•¸ | è¶Šé«˜è¶Šå¥½ï¼Œ>0 è¡¨ç¤ºæ­£å¸¸ |
| **mean_ttft_ms** | é¦– Token æ™‚é–“ | é€šå¸¸ < 1000ms |
| **mean_tpot_ms** | æ¯ Token ç”Ÿæˆæ™‚é–“ | è¶Šä½è¶Šå¥½ |
| **mean_latency_ms** | å¹³å‡å»¶é² | å–æ±ºæ–¼æ¨¡å‹å¤§å° |

**ç•°å¸¸è¨Šè™Ÿ**ï¼š
- throughput = 0 æˆ–éå¸¸ä½
- å»¶é²ç•°å¸¸é«˜ (>10000ms)
- æ¸¬è©¦ä¸­æ–·æˆ–ç„¡çµæœ

---

## ğŸ”§ å¸¸è¦‹å•é¡Œå¿«é€Ÿæ’æŸ¥

### Q1: æ¨¡å‹è¼‰å…¥å¤±æ•—

**éŒ¯èª¤è¨Šæ¯**: `OutOfMemoryError` æˆ– `CUDA/HIP error`

**è§£æ±ºæ–¹æ³•**ï¼š
1. é™ä½è¨˜æ†¶é«”ä½¿ç”¨ç‡ï¼š
   ```bash
   vllm serve google/gemma-3-4b-it \
     --enforce-eager \
     --gpu-memory-utilization 0.7  # é™è‡³ 70%
   ```

2. ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼š
   ```bash
   vllm serve facebook/opt-125m \
     --enforce-eager \
     --tensor-parallel-size 1
   ```

### Q2: GPU ç„¡æ³•è­˜åˆ¥

**éŒ¯èª¤è¨Šæ¯**: `No GPU detected` æˆ– `ROCm not available`

**æª¢æŸ¥æ­¥é©Ÿ**ï¼š
```bash
# æª¢æŸ¥ GPU æ˜¯å¦è¢«ç³»çµ±è­˜åˆ¥
amd-smi

# æª¢æŸ¥ Docker æ˜¯å¦èƒ½è¨ªå• GPU
docker exec vllm-server amd-smi

# æª¢æŸ¥è¨­å‚™æ¬Šé™
ls -l /dev/kfd /dev/dri
```

### Q3: æ¸¬è©¦ä¸­é€”åœæ­¢

**å¯èƒ½åŸå› **ï¼š
- GPU éç†±ä¿è­·
- è¨˜æ†¶é«”ä¸è¶³
- é©…å‹•ä¸ç©©å®š

**æª¢æŸ¥æ–¹æ³•**ï¼š
```bash
# æŸ¥çœ‹ç³»çµ±æ—¥èªŒ
dmesg | tail -50

# æŸ¥çœ‹ Docker æ—¥èªŒ
docker logs vllm-server | tail -100
```

---

## ğŸ¯ å»ºè­°æ¸¬è©¦æ¨¡å‹æ¸…å–®

æ ¹æ“šç¡¬é«”è¦æ ¼é¸æ“‡é©åˆçš„æ¸¬è©¦æ¨¡å‹ï¼š

### å–® GPU æ¸¬è©¦ (VRAM < 32GB)

1. **Gemma 3 4B** - åŸºç¤é©—è­‰
   ```bash
   vllm serve google/gemma-3-4b-it --enforce-eager
   ```

2. **Llama 3.1 8B** - ä¸­ç­‰è² è¼‰
   ```bash
   vllm serve meta-llama/Llama-3.1-8B --enforce-eager
   ```

3. **Qwen 3 8B** - æ›¿ä»£é¸é …
   ```bash
   vllm serve Qwen/Qwen3-8B --enforce-eager
   ```

### å¤š GPU æ¸¬è©¦ (4+ GPUs)

4. **GPT-OSS 120B** - é‡åº¦å£“åŠ›æ¸¬è©¦
   ```bash
   vllm serve openai/gpt-oss-120b \
     --tensor-parallel-size 4 \
     --enforce-eager
   ```

---

## ğŸ”„ ä¸åŒç¡¬é«”çš„æ¸¬è©¦èª¿æ•´

### é‡å°ä¸åŒ GPU æ•¸é‡

| GPU æ•¸é‡ | å»ºè­°æ¸¬è©¦æ¨¡å‹ | tensor-parallel-size |
|---------|-------------|---------------------|
| 1 å¼µ | Gemma 3 4B, Llama 3.1 8B | 1 |
| 2 å¼µ | Llama 3.1 8B | 2 |
| 4 å¼µ | GPT-OSS 120B | 4 |
| 8 å¼µ | GPT-OSS 120B | 8 |

### é‡å°ä¸åŒ VRAM å®¹é‡

| VRAM | å»ºè­°æ¨¡å‹å¤§å° | è¨˜æ†¶é«”ä½¿ç”¨ç‡ |
|------|------------|-------------|
| 16GB | 4B-8B | 0.7 |
| 24GB | 8B-13B | 0.8 |
| 32GB+ | 8B-70B | 0.9 |
| 48GB+ | 70B-120B | 0.9 |

---

## ğŸ“š ç›¸é—œè³‡æº

- **å°ˆæ¡ˆ GitHub**: [https://github.com/hanchi-gao/vllm_t](https://github.com/hanchi-gao/vllm_t)
- **å®Œæ•´æŠ€è¡“æ–‡æª”**: [README.md](README.md)

---

## ğŸ†˜ è¯çµ¡æ”¯æ´

å¦‚æ¸¬è©¦éç¨‹é‡åˆ°å•é¡Œï¼Œè«‹è¯çµ¡ï¼š

**VRD-Henry #11750**

æä¾›ä»¥ä¸‹è³‡è¨Šä»¥ä¾¿å¿«é€Ÿå”åŠ©ï¼š
1. ç¡¬é«”è¦æ ¼è³‡è¨Š (`amd-smi`)
2. éŒ¯èª¤è¨Šæ¯æˆªåœ–
3. Docker æ—¥èªŒ: `docker logs vllm-server`
4. ç³»çµ±æ—¥èªŒ: `dmesg | tail -100`

---

**æ–‡æª”æ›´æ–°æ—¥æœŸ**: 2024-12-24
**é©ç”¨ç‰ˆæœ¬**: vLLM 0.10.2 + ROCm 7.0.0
