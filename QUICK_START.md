# å¿«é€Ÿé–‹å§‹æŒ‡å—

## ğŸš€ å¤šæ©Ÿå™¨æ¸¬è©¦å¿«é€ŸæŒ‡ä»¤

### æ©Ÿå™¨ A/Bï¼ˆ17 å€‹æ¸¬è©¦ï¼‰

```bash
# 1. å•Ÿå‹•ç’°å¢ƒ
cd docker_setup
docker compose -f docker-compose.bench.yml up -d

# 2. åŸ·è¡Œæ¸¬è©¦
docker exec -it vllm-bench-client bash /root/benchmark_tests/scripts/run_machine_AB_tests.sh

# 3. æ‰“åŒ…çµæœï¼ˆå®Œæˆå¾Œï¼‰
tar -czf machine_AB_results.tar.gz bench_results/pcie/
```

**æ¸¬è©¦å…§å®¹**ï¼š
- Group 1: 7B + TP=1 (é…ç½® A+B, 6 æ¸¬è©¦)
- Group 2: 7B + TP=2 (é…ç½® B, 3 æ¸¬è©¦)
- Group 3: 13B + TP=1 (é…ç½® A, 2 æ¸¬è©¦)
- Group 4: 13B + TP=2 (é…ç½® B, 3 æ¸¬è©¦)
- Group 5: 30B + TP=2 (é…ç½® B, 3 æ¸¬è©¦)

---

### æ©Ÿå™¨ Cï¼ˆ12 å€‹æ¸¬è©¦ï¼‰

```bash
# 1. å•Ÿå‹•ç’°å¢ƒ
cd docker_setup
docker compose -f docker-compose.bench.yml up -d

# 2. åŸ·è¡Œæ¸¬è©¦
docker exec -it vllm-bench-client bash /root/benchmark_tests/scripts/run_machine_C_tests.sh

# 3. åˆä½µçµæœï¼ˆå¾æ©Ÿå™¨ A/B è¤‡è£½éä¾†ï¼‰
scp user@machine-ab:/path/to/machine_AB_results.tar.gz .
tar -xzf machine_AB_results.tar.gz
```

**æ¸¬è©¦å…§å®¹**ï¼š
- Group 1: 7B + TP=1 (3 æ¸¬è©¦)
- Group 2: 7B + TP=2 (3 æ¸¬è©¦)
- Group 3: 13B + TP=2 (3 æ¸¬è©¦)
- Group 4: 30B + TP=2 (3 æ¸¬è©¦)

---

## ğŸ“‹ æ¸¬è©¦çµ„åˆé€ŸæŸ¥è¡¨

### æ©Ÿå™¨ A/B æ¸¬è©¦åˆ—è¡¨

| çµ„åˆ¥ | vLLM Server å‘½ä»¤ | æ¸¬è©¦ ID | é…ç½®åˆ‡æ› |
|-----|-----------------|---------|---------|
| **Group 1** | `vllm serve meta-llama/Llama-3.1-8B --tensor-parallel-size 1 --gpu-memory-utilization 0.9 --max-model-len 4096 --enforce-eager` | 1A-1k, 1A-2k, 1A-4k<br>1B-1k, 1B-2k, 1B-4k | A â†’ B |
| **Group 2** | `vllm serve meta-llama/Llama-3.1-8B --tensor-parallel-size 2 --gpu-memory-utilization 0.9 --max-model-len 4096 --enforce-eager` | 2B-1k, 2B-2k, 2B-4k | B |
| **Group 3** | `vllm serve meta-llama/Llama-2-13b-hf --tensor-parallel-size 1 --gpu-memory-utilization 0.9 --max-model-len 4096 --enforce-eager` | 3A-1k, 3A-2k | A |
| **Group 4** | `vllm serve meta-llama/Llama-2-13b-hf --tensor-parallel-size 2 --gpu-memory-utilization 0.9 --max-model-len 4096 --enforce-eager` | 3B-1k, 3B-2k, 3B-4k | B |
| **Group 5** | `vllm serve meta-llama/Llama-2-30b-hf --tensor-parallel-size 2 --gpu-memory-utilization 0.9 --max-model-len 4096 --enforce-eager` | 4B-1k, 4B-2k, 4B-4k | B |

### æ©Ÿå™¨ C æ¸¬è©¦åˆ—è¡¨

| çµ„åˆ¥ | vLLM Server å‘½ä»¤ | æ¸¬è©¦ ID |
|-----|-----------------|---------|
| **Group 1** | `vllm serve meta-llama/Llama-3.1-8B --tensor-parallel-size 1 --gpu-memory-utilization 0.9 --max-model-len 4096 --enforce-eager` | 1C-1k, 1C-2k, 1C-4k |
| **Group 2** | `vllm serve meta-llama/Llama-3.1-8B --tensor-parallel-size 2 --gpu-memory-utilization 0.9 --max-model-len 4096 --enforce-eager` | 2C-1k, 2C-2k, 2C-4k |
| **Group 3** | `vllm serve meta-llama/Llama-2-13b-hf --tensor-parallel-size 2 --gpu-memory-utilization 0.9 --max-model-len 4096 --enforce-eager` | 3C-1k, 3C-2k, 3C-4k |
| **Group 4** | `vllm serve meta-llama/Llama-2-30b-hf --tensor-parallel-size 2 --gpu-memory-utilization 0.9 --max-model-len 4096 --enforce-eager` | 4C-1k, 4C-2k, 4C-4k |

---

## ğŸ”§ vLLM Server æ“ä½œ

### å•Ÿå‹• Serverï¼ˆåœ¨ vllm-server å®¹å™¨å…§ï¼‰

```bash
# é€²å…¥å®¹å™¨
docker exec -it vllm-server bash

# å•Ÿå‹• serverï¼ˆæ ¹æ“šä¸Šè¡¨é¸æ“‡å°æ‡‰å‘½ä»¤ï¼‰
vllm serve <model> --tensor-parallel-size <1|2> --gpu-memory-utilization 0.9 --max-model-len 4096 --enforce-eager
```

### åœæ­¢ Server

```bash
# åœ¨ server å®¹å™¨å…§æŒ‰ Ctrl+C
# æˆ–å¾å¤–éƒ¨å¼·åˆ¶åœæ­¢
docker exec vllm-server pkill -f "vllm serve"
```

### æª¢æŸ¥ Server ç‹€æ…‹

```bash
# å¥åº·æª¢æŸ¥
curl http://localhost:8000/health

# æŸ¥çœ‹è¼‰å…¥çš„æ¨¡å‹
curl http://localhost:8000/v1/models
```

---

## ğŸ“Š æŸ¥çœ‹çµæœ

```bash
# æŸ¥çœ‹æ‰€æœ‰çµæœ
ls -lh bench_results/pcie/

# çµ±è¨ˆæ¸¬è©¦æ•¸é‡
ls bench_results/pcie/*.json | wc -l

# å¿«é€ŸæŸ¥çœ‹å–®å€‹çµæœ
python3 -m json.tool bench_results/pcie/A_7B_TP1_1k_*.json

# æå–é—œéµæŒ‡æ¨™
cat bench_results/pcie/A_7B_TP1_1k_*.json | jq '{
  test_id: .test_metadata.test_id,
  config: .test_metadata.hardware_config,
  throughput: .throughput,
  ttft: .ttft,
  tpot: .tpot
}'
```

---

## âš¡ ä¸¦è¡ŒåŸ·è¡Œç­–ç•¥

å…©å°æ©Ÿå™¨å¯ä»¥**åŒæ™‚åŸ·è¡Œ**ä»¥ç¯€çœæ™‚é–“ï¼š

```
æ™‚é–“è»¸ï¼š
â”‚
â”œâ”€ æ©Ÿå™¨ A/B é–‹å§‹æ¸¬è©¦ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                          â”‚
â”œâ”€ æ©Ÿå™¨ C é–‹å§‹æ¸¬è©¦ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚                                â”‚        â”‚
â”‚                                â”œâ”€ 1-1.5hâ”‚
â”‚                                â”‚        â”‚
â”‚                                çµæŸ â”€â”€â”€â”€â”˜
â”‚                                          â”‚
â”‚                                          â”œâ”€ 1.5-2h
â”‚                                          â”‚
â”‚                                          çµæŸ
â”‚
ç¸½æ™‚é–“ï¼š~2.5 å°æ™‚ï¼ˆä¸¦è¡Œï¼‰ vs ~3.5 å°æ™‚ï¼ˆä¸²è¡Œï¼‰
```

---

## ğŸ†˜ å¸¸è¦‹å•é¡Œ

### Server å•Ÿå‹•å¤±æ•—

```bash
# æª¢æŸ¥ GPU
docker exec vllm-server rocm-smi

# æŸ¥çœ‹è©³ç´°éŒ¯èª¤
docker logs vllm-server

# VRAM ä¸è¶³æ™‚é™ä½ä½¿ç”¨ç‡
vllm serve <model> --gpu-memory-utilization 0.7 --enforce-eager
```

### æ¸¬è©¦è…³æœ¬ä¸­æ–·

```bash
# æ¸¬è©¦çµæœå·²ä¿å­˜ï¼Œå¯ä»¥æ‰‹å‹•ç¹¼çºŒ
# æŸ¥çœ‹å·²å®Œæˆçš„æ¸¬è©¦
ls bench_results/pcie/

# æ‰‹å‹•åŸ·è¡Œå‰©é¤˜æ¸¬è©¦
/root/benchmark_tests/scripts/run_pcie_benchmark.sh --config <X> --model <YB> --tp <Z> --input-len <N>
```

### çµæœæ–‡ä»¶éºå¤±

```bash
# æª¢æŸ¥å®¹å™¨å…§çš„çµæœ
docker exec vllm-bench-client ls -lh /root/bench_results/pcie/

# å¾å®¹å™¨è¤‡è£½å‡ºä¾†
docker cp vllm-bench-client:/root/bench_results/pcie ./bench_results/
```

---

**å®Œæ•´æ–‡æª”**: è«‹åƒè€ƒ [README.md](README.md) å’Œ [test_matrix.md](test_matrix.md)
